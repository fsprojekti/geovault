@article{Tari2006,
abstract = {Previous research has found graphical passwords to be more memorable than non-dictionary or "strong" alphanumeric passwords. Participants in a prior study expressed concerns that this increase in memorability could also lead to an increased susceptibility of graphical passwords to shoulder-surfing. This appears to be yet another example of the classic trade-off between usability and security for authentication systems. This paper explores whether graphical passwords' increased memorability necessarily leads to risks of shoulder-surfing. To date, there are no studies examining the vulnerability of graphical versus alphanumeric passwords to shoulder-surfing. This paper examines the real and perceived vulnerability to shoulder-surfing of two configurations of a graphical password, Passfaces™[30], compared to non-dictionary and dictionary passwords. A laboratory experiment with 20 participants asked them to try to shoulder surf the two configurations of Passfaces™ (mouse versus keyboard data entry) and strong and weak passwords. Data gathered included the vulnerability of the four authentication system configurations to shoulder-surfing and study participants' perceptions concerning the same vulnerability. An analysis of these data compared the relative vulnerability of each of the four configurations to shouldersurfing and also compared study participants' real and perceived success in shoulder-surfing each of the configurations. Further analysis examined the relationship between study participants' real and perceived success in shoulder-surfing and determined whether there were significant differences in the vulnerability of the four authentication configurations to shoulder-surfing. Findings indicate that configuring data entry for Passfaces™ through a keyboard is the most effective deterrent to shouldersurfing in a laboratory setting and the participants' perceptions were consistent with that result. While study participants believed that Passfaces™ with mouse data entry would be most vulnerable to shoulder-surfing attacks, the empirical results found that strong passwords were actually more vulnerable.},
author = {Tari, Furkan and Ozok, A. Ant and Holden, Stephen H.},
doi = {10.1145/1143120.1143128},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tari, Ozok, Holden - 2006 - A comparison of perceived and real shoulder-surfing risks between alphanumeric and graphical passwords.pdf:pdf},
isbn = {1595934480},
journal = {ACM International Conference Proceeding Series},
keywords = {Authentication,Graphical passwords,Human factors,Password security,Shoulder surfing,Social engineering,Usable security},
pages = {56--66},
title = {{A comparison of perceived and real shoulder-surfing risks between alphanumeric and graphical passwords}},
volume = {149},
year = {2006}
}
@article{Das2019,
abstract = {In cryptocurrencies such as Bitcoin or Ethereum, users control funds via secret keys. To transfer funds from one user to another, the owner of the money signs a new transaction that transfers the funds to the new recipient. This makes secret keys a highly attractive target for attacks, and has lead to prominent examples where millions of dollars worth in cryptocurrency have been stolen. To protect against these attacks, a widely used approach are so-called hot/cold wallets. In a hot/cold wallet system, the hot wallet is permanently connected to the network, while the cold wallet stores the secret key and is kept without network connection. In this work, we propose the first comprehensive security model for hot/cold wallets and develop wallet schemes that are provable secure within these models. At the technical level our main contribution is to provide a new provably secure ECDSA-based hot/cold wallet scheme that can be integrated into legacy cryptocurrencies such as Bitcoin. Our construction and security analysis uses a modular approach, where we show how to generically build secure hot/cold wallets from signature schemes that exhibit a rerandomizing property of the keys.},
author = {Das, Poulami and Faust, Sebastian and Loss, Julian},
doi = {10.1145/3319535.3354236},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Das, Faust, Loss - 2019 - A formal treatment of deterministic wallets.pdf:pdf},
isbn = {9781450367479},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Cryptocurrencies,Foundations,Wallets},
month = {nov},
pages = {651--668},
publisher = {Association for Computing Machinery},
title = {{A formal treatment of deterministic wallets}},
url = {https://dl.acm.org/doi/pdf/10.1145/3319535.3354236},
year = {2019}
}
@article{Gasser1975,
abstract = {Technical Report ESD-TR-75-97},
author = {Gasser, Morrie},
doi = {10.21236/ADA017676},
institution = {US Dept of the Air Force},
month = {nov},
pages = {193},
title = {{A Random Word Generator for Pronounceable Passwords}},
url = {http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA017676},
year = {1975}
}
@article{Mai2022,
abstract = {A common need for artificial intelligence models in the broader geoscience is to encode various types of spatial data, such as points, polylines, polygons, graphs, or rasters, in a hidden embedding space so that they can be readily incorporated into deep learning models. One fundamental step is to encode a single point location into an embedding space, such that this embedding is learning-friendly for downstream machine learning models. We call this process location encoding. However, there lacks a systematic review on location encoding, its potential applications, and key challenges that need to be addressed. This paper aims to fill this gap. We first provide a formal definition of location encoding, and discuss the necessity of it for GeoAI research. Next, we provide a comprehensive survey about the current landscape of location encoding research. We classify location encoding models into different categories based on their inputs and encoding methods, and compare them based on whether they are parametric, multi-scale, distance preserving, and direction aware. We demonstrate that existing location encoders can be unified under one formulation framework. We also discuss the application of location encoding. Finally, we point out several challenges that need to be solved in the future.},
archivePrefix = {arXiv},
arxivId = {2111.04006},
author = {Mai, Gengchen and Janowicz, Krzysztof and Hu, Yingjie and Gao, Song and Yan, Bo and Zhu, Rui and Cai, Ling and Lao, Ni},
doi = {10.1080/13658816.2021.2004602},
eprint = {2111.04006},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mai et al. - 2022 - A review of location encoding for GeoAI methods and applications.pdf:pdf},
issn = {13623087},
journal = {International Journal of Geographical Information Science},
keywords = {GeoAI,Location encoding,representation learning,spatially explicit machine learning},
number = {4},
pages = {639--673},
publisher = {Taylor & Francis},
title = {{A review of location encoding for GeoAI: methods and applications}},
url = {https://www.tandfonline.com/doi/pdf/10.1080/13658816.2021.2004602},
volume = {36},
year = {2022}
}
@article{Yang2016,
abstract = {Mnemonic strategy has been recommended to help users generate secure and memorable passwords. We evaluated the security of 6 mnemonic strategy variants in a series of online studies involving 5, 484 participants. In addition to applying the standard method of using guess numbers or similar metrics to compare the generated passwords, we also measured the frequencies of the most commonly chosen sentences as well as the resulting passwords. While metrics similar to guess numbers suggested that all variants provided highly secure passwords, statistical metrics told a different story. In particular, differences in the exact instructions had a tremendous impact on the security level of the resulting passwords. We examined the mental workload and memorability of 2 mnemonic strategy variants in another online study with 752 participants. Although perceived workloads for the mnemonic strategy variants were higher than that for the control group where no strategy is required, no significant reduction in password recall after 1 week was obtained.},
author = {Yang, Weining and Li, Ninghui and Chowdhury, Omar and Xiong, Aiping and Proctor, Robert W.},
doi = {10.1145/2976749.2978346},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2016 - An empirical study of mnemonic sentence-based password generation strategies.pdf:pdf},
isbn = {9781450341394},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
month = {oct},
pages = {1216--1229},
publisher = {Association for Computing Machinery},
title = {{An empirical study of mnemonic sentence-based password generation strategies}},
url = {https://dl.acm.org/doi/pdf/10.1145/2976749.2978346},
volume = {24-28-October-2016},
year = {2016}
}
@article{DiLuzio2020,
abstract = {This work presents Arcula, a new design for hierarchical deterministic wallets that brings identity-based public keys to the blockchain. Arcula is built on top of provably secure cryptographic primitives. It generates all its cryptographic secrets from a user-provided seed and enables the derivation of new public keys based on the identities of users, without requiring any secret information. Unlike other wallets, it achieves all these properties while being secure against privilege escalation. We formalize the security model of hierarchical deterministic wallets and prove that an attacker compromising an arbitrary number of users within an Arcula wallet cannot escalate his privileges and compromise users higher in the access hierarchy. Our design works out-of-the-box with any blockchain that enables the verification of signatures on arbitrary messages. We evaluate its usage in a real-world scenario on the Bitcoin Cash network.},
archivePrefix = {arXiv},
arxivId = {1906.05919},
author = {{Di Luzio}, Adriano and Francati, Danilo and Ateniese, Giuseppe},
doi = {10.1007/978-3-030-65411-5_16},
eprint = {1906.05919},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Di Luzio, Francati, Ateniese - 2020 - Arcula A secure hierarchical deterministic wallet for multi-asset blockchains.pdf:pdf},
isbn = {9783030654108},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bitcoin,Blockchain,Hierarchical deterministic wallet,Hierarchical key assignment},
pages = {323--343},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Arcula: A secure hierarchical deterministic wallet for multi-asset blockchains}},
url = {https://link.springer.com/chapter/10.1007/978-3-030-65411-5_16},
volume = {12579 LNCS},
year = {2020}
}
@article{Kavrestad2020,
abstract = {In this paper, we describe and evaluate how the learning framework ContextBased MicroTraining (CBMT) can be used to assist users to create strong passwords. Rather than a technical enforcing measure, CBMT is a framework that provides information security training to users when they are in a situation where the training is directly relevant. The study is carried out in two steps. First, a survey is used to measure how well users understand password guidelines that are presented in different ways. The second part measures how using CBMT to present password guidelines affect the strength of the passwords created. This experiment was carried out by implementing CBMT at the account registration page of a local internet service provider and observing the results on user-created passwords. The results of the study show that users presented with passwords creation guidelines using a CBMT learning module do understand the password creation guidelines to a higher degree than other users. Further, the experiment shows that users presented with password guidelines in the form of a CBMT learning module do create passwords that are longer and more secure than other users. The assessment of password security was performed using the zxcvbn tool, developed by Dropbox, that measures password entropy.},
author = {K{\"{a}}vrestad, Joakim and Nohlberg, Marcus},
doi = {10.1007/978-3-030-58201-2_7},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/K{\"{a}}vrestad, Nohlberg - 2020 - Assisting Users to Create Stronger Passwords Using ContextBased MicroTraining.pdf:pdf},
isbn = {9783030582005},
issn = {1868422X},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {CBMT,ContextBased MicroTraining,Passwords,Security training},
pages = {95--108},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Assisting Users to Create Stronger Passwords Using ContextBased MicroTraining}},
volume = {580 IFIP},
year = {2020}
}
@article{Turuani2016,
abstract = {We introduce a formal modeling in ASLan++ of the twofactor authentication protocol used by the Electrum Bitcoin wallet. This allows us to perform an automatic analysis of the wallet and show that it is secure for standard scenarios in Dolev Yao model [Dolev 1981]. The result could be derived thanks to some advanced features of the protocol analyzer such as the possibility to specify (i) new intruder deduction rules with clauses and (ii) non-deducibility constraints.},
author = {Turuani, Mathieu and Voegtlin, Thomas and Rusinowitch, Michael},
doi = {10.1007/978-3-662-53357-4_3},
isbn = {9783662533567},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {27--42},
publisher = {Springer Verlag},
title = {{Automated verification of electrum wallet}},
volume = {9604 LNCS},
year = {2016}
}
@inproceedings{8726762,
abstract = {Cryptocurrency wallets store the wallet's private key(s), and hence, are a lucrative target for attackers. With possession of the private key, an attacker virtually owns all of the currency in the compromised wallet. Managing cryptocurrency wallets offline, in isolated ('air-gapped') computers, has been suggested in order to secure the private keys from theft. Such air-gapped wallets are often referred to as 'cold wallets.' In this paper we show how private keys can be exfiltrated from air-gapped wallets. In the adversarial attack model, the attacker infiltrates the offline wallet, infecting it with malicious code. The malware can be preinstalled or pushed in during the initial installation of the wallet, or it can infect the system when removable media (e.g., USB flash drive) is inserted into the wallet's computer in order to sign a transaction. These attack vectors have repeatedly been proven feasible in the last decade (e.g., [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]). Having obtained a foothold in the wallet, an attacker can utilize various air-gap covert channel techniques (bridgeware [11]) to jump the airgap and exfiltrate the wallet's private keys. We evaluate various exfiltration techniques, including physical, electromagnetic, electric, magnetic, acoustic, optical, and thermal techniques. This research shows that although cold wallets provide a high degree of isolation, it's not beyond the capability of motivated attackers to compromise such wallets and steal private keys from them. We demonstrate how a 256-bit private key (e.g., Bitcoin's private keys) can be exfiltrated from an offline, air-gapped wallet of a fictional character named Satoshi within a matter of seconds.},
archivePrefix = {arXiv},
arxivId = {1804.08714},
author = {Guri, Mordechai},
booktitle = {Proceedings - IEEE 2018 International Congress on Cybermatics: 2018 IEEE Conferences on Internet of Things, Green Computing and Communications, Cyber, Physical and Social Computing, Smart Data, Blockchain, Computer and Information Technology, iThings/GreenCom/CPSCom/SmartData/Blockchain/CIT 2018},
doi = {10.1109/Cybermatics_2018.2018.00227},
eprint = {1804.08714},
isbn = {9781538679753},
keywords = {Bitcoin;Malware;Media;Hardware;Universal Serial Bu},
pages = {1308--1316},
title = {{Beatcoin: Leaking private keys from air-gapped cryptocurrency wallets}},
year = {2018}
}
@article{McEvoy2016,
abstract = {Our society depends on password-based authentication methods for accessing valuable information. However, the use of weak passwords is placing us at risk. Cyber security systems encourage users to employ strong passwords often by increasing requirements. Unfortunately, using a strong password requires more cognitive effort. This increase in effort pushes users to find workarounds that directly harm security. The paradox between security and usability has often resulted in simply blaming users rather than seeking a Human-Centered Design perspective. We introduce a strategy for developing strong passwords that embed contextual cues within mnemonic phrase passwords. Using this strategy participants were able to create strong passwords and better remember them compared with a traditional mnemonic strategy.},
author = {McEvoy, Pete and Still, Jeremiah D.},
doi = {10.1007/978-3-319-41932-9_24},
isbn = {9783319419312},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Authentication,Human memory,Human-centered-design,Usable security},
pages = {295--304},
publisher = {Springer, Cham},
title = {{Contextualizing mnemonic phrase passwords}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-41932-9_24},
volume = {501},
year = {2016}
}
@article{Erwig2022,
abstract = {Adaptor signatures are a new cryptographic primitive that binds the authentication of a message to the revelation of a secret value. In recent years, this primitive has gained increasing popularity both in academia and practice due to its versatile use-cases in different Blockchain applications such as atomic swaps and payment channels. The security of these applications, however, crucially relies on users storing and maintaining the secret values used by adaptor signatures in a secure way. For standard digital signature schemes, cryptographic wallets have been introduced to guarantee secure storage of keys and execution of the signing procedure. However, no prior work has considered cryptographic wallets for adaptor signatures. In this work, we introduce the notion of adaptor wallets. Adaptor wallets allow parties to securely use and maintain adaptor signatures in the Blockchain setting. Our adaptor wallets are both deterministic and operate in the hot/cold paradigm, which was first formalized by Das et al. (CCS 2019) for standard signature schemes. We introduce a new cryptographic primitive called adaptor signatures with rerandomizable keys, and use it to generically construct adaptor wallets. We further show how to instantiate adaptor signatures with rerandomizable keys from the ECDSA signature scheme and discuss that they can likely be built for Schnorr and Katz-Wang schemes as well. Finally, we discuss the limitations of the existing ECDSA- and Schnorr-based adaptor signatures w.r.t. deterministic wallets in the hot/cold setting and prove that it is impossible to overcome these drawbacks given the current state-of-the-art design of adaptor signatures.},
author = {Erwig, Andreas and Riahi, Siavash},
doi = {10.1007/978-3-031-17146-8_24},
isbn = {9783031171451},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {487--506},
publisher = {Springer, Cham},
title = {{Deterministic Wallets for Adaptor Signatures}},
url = {https://link.springer.com/chapter/10.1007/978-3-031-17146-8_24},
volume = {13555 LNCS},
year = {2022}
}
@article{Golla2017,
abstract = {—Mobile devices, such as smartphones and tablets, frequently store confidential data, yet implementing a secure device unlock functionality is non-trivial due to restricted input methods. Graphical knowledge-based schemes have been widely used on smartphones and are generally well adapted to the touch-screen interface on small screens. Recently, graphical password schemes based on emoji have been proposed. They offer potential benefits due to the familiarity of users with emoji and the ease of expressing memorable stories. However, it is well-known from other graphical schemes that user-selected authentication secrets can substantially limit the resulting entropy of the authentication secret. In this work, we study the entropy of user-selected secrets for one exemplary instantiation of emoji-based authentication. We analyzed an implementation using 20 emoji displayed in random order on a grid, where a user selects passcodes of length 4 without further restrictions. We conducted an online user study with 795 participants, using the collected passcodes to determine the resistance to guessing based on several guessing strategies, thus estimating the selection bias. We evaluated Markov model-based guessing strategies based on the selected sequence of emoji, on its position in the grid, and combined models taking into account both features. While we find selection bias based on both the emoji as well as the position, the measured bias is lower than for similar schemes. Depending on the model, we can recover up to 7 % at 100 guessing attempts, and up to 11 % of the passcodes at 1 000 guessing attempts. (For comparison, previous work on the graphical Android Unlock pattern scheme (CCS 2013) recovered around 18 % at 100 and 50 % at 1 000 guessing attempts, despite a theoretical keyspace of more than double the size for the Android scheme.) These results demonstrate some potential for a usable and relatively secure scheme and show that the size of the theoretical keyspace is a bad predictor for the realistic guessability of passcodes.},
author = {Golla, Maximilian and Detering, Dennis and Drmut, Markus},
doi = {10.14722/usec.2017.23024},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Golla, Detering, D{\"{u}}rmuth - 2017 - EmojiAuth Quantifying the Security of Emoji-based Authentication.pdf:pdf},
title = {{EmojiAuth: Quantifying the Security of Emoji-based Authentication}},
url = {http://dx.doi.org/10.14722/usec.2017.23024},
year = {2017}
}
@article{Lindell2018,
abstract = {ECDSA is a standardized signing algorithm that is widely used in TLS, code signing, cryptocurrency and more. Due to its importance, the problem of securely computing ECDSA in a distributed manner (known as threshold signing) has received considerable interest. However, despite this interest, there is still no full threshold solution for more than 2 parties (meaning that any t-out-of-n parties can sign, security is preserved for any t - 1 or fewer corrupted parties, and t = n can be any value thus supporting an honest minority) that has practical key distribution. This is due to the fact that all previous solutions for this utilize Paillier homomorphic encryption, and efficient distributed Paillier key generation for more than two parties is not known. In this paper, we present the first truly practical full threshold ECDSA signing protocol that has both fast signing and fast key distribution. This solves a years-old open problem, and opens the door to practical uses of threshold ECDSA signing that are in demand today. One of these applications is the construction of secure cryptocurrency wallets (where key shares are spread over multiple devices and so are hard to steal) and cryptocurrency custody solutions (where large sums of invested cryptocurrency are strongly protected by splitting the key between a bank/financial institution, the customer who owns the currency, and possibly a third-party trustee, in multiple shares at each). There is growing practical interest in such solutions, but prior to our work these could not be deployed today due to the need for distributed key generation.},
author = {Lindell, Yehuda and Nof, Ariel},
doi = {10.1145/3243734.3243788},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindell, Nof - 2018 - Fast secure multiparty ECDSA with practical distributed key generation and applications to cryptocurrency custody.pdf:pdf},
isbn = {9781450356930},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
month = {oct},
pages = {1837--1854},
publisher = {Association for Computing Machinery},
title = {{Fast secure multiparty ECDSA with practical distributed key generation and applications to cryptocurrency custody}},
year = {2018}
}
@article{Karantias2020,
abstract = {The Bitcoin scheme is a rare example of a large scale global payment system in which all the transactions are publicly accessible (but in an anonymous way). We downloaded the full history of this scheme, and analyzed many statistical properties of its associated transaction graph. In this paper we answer for the first time a variety of interesting questions about the typical behavior of users, how they acquire and how they spend their bitcoins, the balance of bitcoins they keep in their accounts, and how they move bitcoins between their various accounts in order to better protect their privacy. In addition, we isolated all the large transactions in the system, and discovered that almost all of them are closely related to a single large transaction that took place in November 2010, even though the associated users apparently tried to hide this fact with many strange looking long chains and fork-merge structures in the transaction graph.},
address = {Berlin, Heidelberg},
author = {Karantias, Kostis and Kiayias, Aggelos and Zindros, Dionysis},
doi = {10.1007/978-3-662-54970-4},
editor = {Grossklags, Jens and Preneel, Bart},
isbn = {978-3-642-39883-4},
keywords = {bitcoin,digital coins,electronic cash,payment systems,quantitative analysis,trans-action graphs},
pages = {523--540},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Financial Cryptography and Data Security 2013}},
url = {https://dx.doi.org/10.1007/978-3-642-39884-1},
volume = {9603},
year = {2020}
}
@article{Boers2005,
abstract = {The Lexical Approach (LA) is founded on the belief that, in order to achieve a high level of accuracy with fluency, learners of a foreign language need to commit to memory vast numbers of multi-word expressions. However, since it is far from clear that the methodology currently associated with the LA holds out well-founded hope that phrase learning on such a vast scale is widely achievable in ordinary school-based foreign language teaching, supplementary methods and strategies seem to be called for. It is also possible that a change is needed concerning the criteria for deciding which multi-word expressions should be focused on in teaching and materials writing. Thus far, the criteria most commonly advocated seem to be those of frequency and utility. If it were the case that certain moderately common classes of multi-word lexis were relatively easy to remember on account of salient phonological patterning (e.g., alliteration), it could then be argued that a third criterion, that of memorability, ought to be given additional weight. This paper reports results of controlled experiments which indicate that multi-word expressions which alliterate are indeed more memorable than ones which show no such salient phonological patterning. Implications for FLT methodology are briefly outlined. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Boers, Frank and Lindstromberg, Seth},
doi = {10.1016/j.system.2004.12.007},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boers, Lindstromberg - 2005 - Finding ways to make phrase-learning feasible The mnemonic effect of alliteration.pdf:pdf},
issn = {0346251X},
journal = {System},
keywords = {Alliteration,Foreign language teaching,Lexical approach,Memory,Mnemonic effect,Phonological patterning},
month = {jun},
number = {2},
pages = {225--238},
publisher = {Pergamon},
title = {{Finding ways to make phrase-learning feasible: The mnemonic effect of alliteration}},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X0500014X},
volume = {33},
year = {2005}
}
@article{Mogire2018,
abstract = {Individual security behavior plays a central role in achieving secure computing. However, secure usage is difficult to guarantee in an open-ended context where different users have different perceptions of security as well as different cognitive loads when using security tools. In designing secure systems, it is not only necessary to define secure behavior but also to provide built-in support for such behavior in order to enable users to be complaint. In this work, we explore the viability of augmented cognition as a modality that can be used to support security-oriented behavior in authentication systems. Specifically, we explore how transformations of password character properties such as font and weight can improve password recall and recognition and reduce insecure habits, such as writing down passwords. In a previous study, we tested the accuracy of recall and recognition in an augmented password system. The system was designed to make use of character property transformations to minimize the need for complex passwords while not compromising security. Here we repeat the study, incorporating the use of neurophysiological measures to study human physiological responses during recognition and recall of character sets with different types of transformation. The results suggest that cognitive effort in recall of complex passwords can be alleviated with the performance of the augmented password task. This finding has important implications for future research.},
author = {Mogire, Nancy and Ogawa, Michael Brian and Minas, Randall K. and Auernheimer, Brent and Crosby, Martha E.},
doi = {10.1007/978-3-319-91467-1_11},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mogire et al. - 2018 - Forget the password Password memory and security applications of augmented cognition.pdf:pdf},
isbn = {9783319914664},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Augmented cognition,Authentication,Cybersecurity,Password memory,Physiological measures,Recall,Recognition},
pages = {133--142},
publisher = {Springer Verlag},
title = {{Forget the password: Password memory and security applications of augmented cognition}},
volume = {10916 LNAI},
year = {2018}
}
@article{Li2025,
abstract = {Stealth Address serves as a widely adopted privacy-preserving technique for blockchain systems. However, Liu et al. (EuroS&P 2019) identified a security flaw in the existing Stealth Address algorithms, potentially compromising both the master secret key and derived secret keys. To address these vulnerabilities, Liu et al. (EuroS&P 2019) proposed the Key-Insulated&Privacy-Preserving Signature Scheme with Publicly Derived Public Key (PDPKS), designed to meet the functionality, security, and privacy requirements of Stealth Address. In this paper, we modularly analyze the PDPKS scheme and present a generic construction by integrating a Deterministic Public Key Encryption scheme with a Signature scheme using Rerandomizable Keys. We provide rigorous proof of security and privacy for this construction in the Random Oracle Model. With our construction, the security requirements (Anonymity under Chosen-Ciphertext Attacks and Indistinguishability under Chosen-Ciphertext Attacks) of the underlying PKE scheme in existing constructions can be relaxed to Weak Anonymity.},
author = {Li, Ziyi and Wang, Ruida and Lu, Xianhui and Cheng, Yao and Gao, Liming},
doi = {10.1007/978-981-96-4731-6_6},
isbn = {9789819647309},
issn = {16113349},
journal = {Lecture Notes in Computer Science},
keywords = {PDPKS,Signature with Rerandomizable Keys,Stealth Address},
pages = {106--125},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{From Signature with Re-randomizable Keys: Generic Construction of PDPKS}},
volume = {15543 LNCS},
year = {2025}
}
@article{Gutoski2015,
abstract = {A Bitcoin wallet is a set of private keys known to a user and which allow that user to spend any Bitcoin associated with those keys. In a hierarchical deterministic (HD) wallet, child private keys are generated pseudorandomly from a master private key, and the corresponding child public keys can be generated by anyone with knowledge of the master public key. These wallets have several interesting applications including Internet retail, trustless audit, and a treasurer allocating funds among departments. A specification of HD wallets has even been accepted as Bitcoin standard BIP32. Unfortunately, in all existing HD wallets—including BIP32 wallets— an attacker can easily recover the master private key given the master public key and any child private key. This vulnerability precludes use cases such as a combined treasurer-auditor, and some in the Bitcoin community have suspected that this vulnerability cannot be avoided. We propose a new HD wallet that is not subject to this vulnerability. Our HD wallet can tolerate the leakage of up to m private keys with a master public key size of O(m). We prove that breaking our HD wallet is at least as hard as the so-called “one more” discrete logarithm problem.},
author = {Gutoski, Gus and Stebila, Douglas},
doi = {10.1007/978-3-662-47854-7_31},
isbn = {9783662478530},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {497--504},
publisher = {Springer Verlag},
title = {{Hierarchical deterministic bitcoin wallets that tolerate key leakage}},
volume = {8975},
year = {2015}
}
@article{Bucciarelli2007,
abstract = {In this paper, I present a framework where possible relations between learning and mental models are explored. In particular, I'll be concerned with non-symbolic gestures accompanying discourse and their role in inducing the construction of models and therefore deep comprehension and learning in the listener. Also, I'll be concerned with cognitive and socio-cognitive conflicts and their roles in inducing construction of alternative models of a problem and therefore in learning to reason. Human ability to learn is of great importance for individuals interested in change. Indeed, to learn both declarative and procedural knowledge means to change, and in order to be able to intervene on change in a desired way it is necessary to have a theory of the mental representations and processes involved in learning and a theory of the communication and contexts that favour learning. {\textcopyright} Fondazione Rosselli 2007.},
author = {Bucciarelli, Monica},
doi = {10.1007/s11299-006-0026-y},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bucciarelli - 2007 - How the construction of mental models improves learning.pdf:pdf},
issn = {15937879},
journal = {Mind and Society},
keywords = {Cognitive conflict,Gestures,Learning,Mental models},
month = {jun},
number = {1},
pages = {67--89},
title = {{How the construction of mental models improves learning}},
volume = {6},
year = {2007}
}
@article{Kuo2006,
abstract = {Textual passwords are often the only mechanism used to authenticate users of a networked system. Unfortunately, many passwords are easily guessed or cracked. In an attempt to strengthen passwords, some systems instruct users to create mnemonic phrase-based passwords. A mnemonic password is one where a user chooses a memorable phrase and uses a character (often the first letter) to represent each word in the phrase. In this paper, we hypothesize that users will select mnemonic phrases that are commonly available on the Internet, and that it is possible to build a dictionary to crack mnemonic phrase-based passwords. We conduct a survey to gather user-generated passwords. We show the majority of survey respondents based their mnemonic passwords on phrases that can be found on the Internet, and we generate a mnemonic password dictionary as a proof of concept. Our 400,000-entry dictionary cracked 4% of mnemonic passwords; in comparison, a standard dictionary with 1.2 million entries cracked 11% of control passwords. The user-generated mnemonic passwords were also slightly more resistant to brute force attacks than control passwords. These results suggest that mnemonic passwords may be appropriate for some uses today. However, mnemonic passwords could become more vulnerable in the future and should not be treated as a panacea.},
author = {Kuo, Cynthia and Romanosky, Sasha and Cranor, Lorrie Faith},
doi = {10.1145/1143120.1143129},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Romanosky, Cranor - 2006 - Human selection of mnemonic phrase-based passwords.pdf:pdf},
isbn = {1595934480},
journal = {ACM International Conference Proceeding Series},
keywords = {Mnemonic phrases,Password cracking,Password selection,User studies},
pages = {67--78},
title = {{Human selection of mnemonic phrase-based passwords}},
url = {https://dl.acm.org/doi/pdf/10.1145/1143120.1143129},
volume = {149},
year = {2006}
}
@article{Komiya2021,
abstract = {Password authentication is the most commonly used mechanism for user authentication. However, its vulnerability to different attacks such as dictionary attacks or brute force attack is well known. The users often use password authentication in insecure ways, such as using weak passwords or reusing passwords, which leads to password crackings. Though these problems are apparent, the trade-offs between password strength and password memorability prevent users from using strong passwords. To realize high password strength and memorability, the use of mnemonic passwords is suggested. However, due to its characteristic that the users must use English sentences, this password-generation strategy is not widely used in countries such as Japan, which do not use English as their native language. Therefore, we introduce Japanese mnemonic passwords, which are passwords using password-generation techniques optimized for Japanese users. We conducted a user study to explore the memorability of Japanese mnemonic passwords. We discuss the types of errors made by the participants and how Japanese mnemonic passwords' usability can be enhanced. We also discuss how this strategy can be used in other non-English-speaking countries.},
author = {Komiya, Kosuke and Nakajima, Tatsuo},
doi = {10.1007/978-3-030-77074-7_32},
isbn = {9783030770730},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Japanese,Mnemonic password},
pages = {420--429},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Memorability of Japanese Mnemonic Passwords}},
volume = {12771 LNCS},
year = {2021}
}
@article{Iani2019,
abstract = {Several theoretical approaches suggest that language comprehension and action observation rely on similar mental simulations. Granted that these two simulations partially overlap, we assumed that simulations stemming from action observations are more direct than those stemming from action phrases. The implied prediction was that simulation from action observation should prevail on simulation from action phrases when their effects are contrasted. The results of three experiments confirmed that, when at encoding the phrases were paired with pictures of actions whose kinematics was incongruent with the implied kinematics of the actions described in the phrases, memory for action phrases was impaired (Experiment 1). However, the reverse was not true: when the pictures were paired with phrases representing actions whose kinematics were incongruent with the kinematics of the actions portrayed in the pictures, memory for pictures portraying actions was not impaired (Experiment 2). Also, in line with evidence that simulations from action phrases and those from action observation partially overlap, when their effects were not contrasted their products were misrecognized. In our experiments, when action phrases only presented at recognition described actions depicted in pictures seen at encoding, they were misrecognized as had already been read at encoding (Experiment 1); further, when pictures only presented at recognition portrayed actions described in phrases presented at encoding, they were misrecognized as seen at encoding (Experiment 2). A third experiment excluded the possibility that the pattern of findings was simply a consequence of better memory for pictures of actions as opposed to memory for action phrases (Experiment 3). The implications of our results in relation to the literature on simulation in language comprehension and action observation are discussed.},
author = {Ian{\`{i}}, Francesco and Foiadelli, Andrea and Bucciarelli, Monica},
doi = {10.1016/j.actpsy.2019.01.012},
issn = {00016918},
journal = {Acta Psychologica},
keywords = {Kinematic mental simulations,Memory for action phrases,Memory for action pictures},
month = {mar},
pages = {37--50},
pmid = {30739013},
publisher = {North-Holland},
title = {{Mnemonic effects of action simulation from pictures and phrases}},
url = {https://www.sciencedirect.com/science/article/pii/S0001691818303330},
volume = {194},
year = {2019}
}
@article{Khade2023,
abstract = {Blockchain has grown for various IT businesses and an exchange of cryptocurrencies. The cryptocurrency wallets are essential for storing and managing the digital assets and cryptocurrencies, especially for mobile users. However, the security concerns of the mobile devices and wallets hinder the wide adoption of blockchain technology on mobile devices. Therefore, there is a need of a highly secured mobile wallet for blockchain technology. To address this issue, this paper proposes an architecture for key management in blockchain based mobile wallet. It involves mobile SIM card, as a hardware component and mnemonic phrase to achieve Two-Factor Authentication (2FA). It applies Shamir's secrete sharing scheme in order to enhance the wallet security and robustness. The security analysis of the proposed architecture is performed by considering different attacks.},
author = {Khade, Atharva Vijay and Patel, Harsh Rajesh and Modi, Chirag},
doi = {10.1109/ICBDS58040.2023.10346266},
isbn = {9798350333763},
journal = {2023 IEEE International Conference on Blockchain and Distributed Systems Security, ICBDS 2023},
keywords = {Blockchain,Mobile wallet,Security,Shamir's secret sharing,Two-Factor authentication},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Mnemonic Phrase Management and SIM Based Two-Factor Authentication (2FA) for Mobile Wallets in Blockchain}},
year = {2023}
}
@article{Eleshin2025,
abstract = {Cryptocurrency adoption has surged dramatically, with over 500 million global users. Despite the appeal of self-custodial wallets, which grant users control over their assets, these users often struggle with the complexities of securing seed phrases, leading to substantial financial losses. This paper investigates the behaviors, challenges, and security practices of cryptocurrency users regarding seed phrase management. We conducted a mixed-methods study comprising semi-structured interviews with 20 participants and a comprehensive survey of 643 respondents. Our findings reveal significant gaps in users' understanding and practices around seed phrase security and the circumstances under which users share their seed phrases. We also explore users' mental models of shared accounts and strategies for handling cryptocurrency assets in the event of death. We found that the majority of our participants harbored significant misconceptions about seed phrases that could expose them to significant security risks - e.g., only 43% could correctly identify an image of a seed phrase, many believed they could reset their seed phrase if they lost them. Moreover, only a minority have engaged in any estate planning for their crypto assets. By identifying these challenges and behaviors, we provide actionable insights for the design of more secure and user-friendly cryptocurrency wallets, ultimately aiming to enhance user confidence in managing their crypto assets reduce their exposure to scams and accidental loss of assets, and simplify the creation of bequeathment plans.},
author = {Eleshin, Farida and Sun, Qi and Ye, Mengzhe and Das, Sauvik and Hong, Jason I.},
doi = {10.1145/3706598.3713209},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eleshin et al. - 2025 - Of Secrets and Seedphrases Conceptual Misunderstandings and Security Challenges for Seed Phrase Management among.pdf:pdf},
isbn = {9798400713941},
journal = {Conference on Human Factors in Computing Systems - Proceedings },
keywords = {backups,cryptocurrency wallets,custodial,non-custodial,private key,seedphrase,usability},
month = {apr},
publisher = {Association for Computing Machinery},
title = {{Of Secrets and Seedphrases: Conceptual Misunderstandings and Security Challenges for Seed Phrase Management among Cryptocurrency Users}},
year = {2025}
}
@article{Wang2024,
abstract = {With the widespread adoption and increasing application of blockchain technology, cryptocurrency wallets used in Bitcoin and Ethereum play a crucial role in facilitating decentralized asset management and secure transactions. However, wallet security relies heavily on private keys, with insufficient attention to the risks of theft and exposure. To address this issue, Chaum et al. (ACNS'21) proposed a “proof of ownership” method using a “backup key” to prove ownership of private keys even when exposed. However, their interactive proof approach is inefficient in large-scale systems and vulnerable to side-channel attacks due to the long key generation time. Other related schemes also suffer from low efficiency and complex key management, increasing the difficulty of securely storing backup keys. In this paper, we present an efficient, non-interactive proof generation approach for ownership of secret keys using a single backup key. Our approach leverages non-interactive zero-knowledge proofs and symmetric encryption, allowing users to generate multiple proofs with one fixed backup key, simplifying key management. Additionally, our scheme resists quantum attacks and provides a fallback signature. Our new scheme can be proved to capture unforgeability under the computational indistinguishability from the Uniformly Random Distribution property of a proper hash function and soundness in the quantum random oracle model. Experimental results indicate that our approach achieves a short key generation time and enables an efficient proof generation scheme in large-scale decentralized systems. Compared with state-of-the-art schemes, our approach is applicable to a broader range of scenarios due to its non-interactive nature, short key generation time, high efficiency, and simplified key management system.},
author = {Wang, Chen and Liu, Zi Yuan and Mambo, Masahiro},
doi = {10.3390/cryptography8040057},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Liu, Mambo - 2024 - On the Proof of Ownership of Digital Wallets.pdf:pdf},
issn = {2410387X},
journal = {Cryptography},
keywords = {digital signature,digital wallet,proof of ownership,symmetric encryption,zero-knowledge proof},
month = {dec},
number = {4},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
title = {{On the Proof of Ownership of Digital Wallets}},
volume = {8},
year = {2024}
}
@article{Golla2016,
abstract = {Password vaults are used to store login credentials, usually encrypted by a master password, relieving the user from memorizing a large number of complex passwords. To manage accounts on multiple devices, vaults are often stored at an online service, which substantially increases the risk of leaking the (encrypted) vault. To protect the master password against guessing attacks, previous work has introduced cracking-resistant password vaults based on Honey Encryption. If decryption is attempted with a wrong master password, they output plausible-looking decoy vaults, thus seemingly disabling offline guessing attacks. In this work, we propose attacks against cracking-resistant password vaults that are able to distinguish between real and decoy vaults with high accuracy and thus circumvent the offered protection. These attacks are based on differences in the generated distribution of passwords, which are measured using Kullback-Leibler divergence. Our attack is able to rank the correct vault into the 1.3 % most likely vaults (on median), compared to 37.8 % of the best-reported attack in previous work. (Note that smaller ranks are better, and 50% is achievable by random guessing.) We demonstrate that this attack is, to a certain extent, a fundamental problem with all static Natural Language Encoders (NLE), where the distribution of decoy vaults is fixed. We propose the notion of adaptive NLEs and demonstrate that they substantially limit the effectiveness of such attacks. We give one example of an adaptive NLE based on Markov models and show that the attack is only able to rank the decoy vaults with a median rank of 35.1 %.},
author = {Golla, Maximilian and Beuscher, Benedict and D{\"{u}}rmuth, Markus},
doi = {10.1145/2976749.2978416},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Golla, Beuscher, D{\"{u}}rmuth - 2016 - On the security of cracking-resistant password vaults.pdf:pdf},
isbn = {9781450341394},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Cracking-resistance,Honey Encryption,Natural Language Encoders,Password managers},
month = {oct},
pages = {1230--1241},
publisher = {Association for Computing Machinery},
title = {{On the security of cracking-resistant password vaults}},
volume = {24-28-October-2016},
year = {2016}
}
@article{Blackshear2021,
abstract = {We present a novel approach for blockchain asset owners to reclaim their funds in case of accidental private-key loss or transfer to a mistyped address. Our solution can be deployed upon failure or absence of proactively implemented backup mechanisms, such as secret sharing and cold storage. The main advantages against previous proposals is it does not require any prior action from users and works with both single-key and multi-sig accounts. We achieve this by a 3-phase Commit() → Reveal() → Claim() - or- Challenge() smart contract that enables accessing funds of addresses for which the spending key is not available. We provide an analysis of the threat and incentive models and formalize the concept of reactive KEy-Loss Protection (KELP).},
author = {Blackshear, Sam and Chalkias, Konstantinos and Chatzigiannis, Panagiotis and Faizullabhoy, Riyaz and Khaburzaniya, Irakliy and Kogias, Eleftherios Kokoris and Lind, Joshua and Wong, David and Zakian, Tim},
doi = {10.1007/978-3-662-63958-0_34},
isbn = {9783662639573},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Blockchain,Commitment scheme,Front-running,Key management,Key-loss protection,Smart contracts},
pages = {431--450},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Reactive Key-Loss Protection in Blockchains}},
volume = {12676 LNCS},
year = {2021}
}
@article{Ateniese2005,
abstract = {We introduce the notion of sanitizable signatures that offer many attractive security features for certain current and emerging applications. A sanitizable signature allows authorized semi-trusted censors to modify - in a limited and controlled fashion - parts of a signed message without interacting with the original signer. We present constructions for this new primitive, based on standard signature schemes and secure under common cryptographic assumptions. We also provide experimental measurements for the implementation of a sanitizable signature scheme and demonstrate its practicality. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Ateniese, Giuseppe and Chou, Daniel H. and {De Medeiros}, Breno and Tsudik, Gene},
doi = {10.1007/11555827_10},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ateniese et al. - 2005 - Sanitizable signatures.pdf:pdf},
isbn = {3540289631},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {159--177},
title = {{Sanitizable signatures}},
volume = {3679 LNCS},
year = {2005}
}
@incollection{Das2024,
abstract = {Cryptographic wallets are an essential tool in Blockchain networks to ensure the secure storage and maintenance of an user's cryptographic keys. Broadly, wallets can be divided into three categories, namely custodial, non-custodial, and shared-custodial wallets. The first two are centralized solutions, i.e., the wallet is operated by a single entity, which inherently introduces a single point of failure. Shared-custodial wallets, on the other hand, are maintained by two independent parties, e.g., the wallet user and a service provider, and hence avoid the single point of failure centralized solutions. Unfortunately, current shared-custodial wallets suffer from significant privacy issues. In our work, we introduce password-authenticated deterministic wallets (PADW), a novel and efficient shared-custodial wallet solution, which exhibits strong security and privacy guarantees. In a nutshell, in a PADW scheme, the secret key of the user is shared between the user and the server. In order to generate a signature, the user first authenticates itself to the server by providing a password and afterwards engages in an interactive signing protocol with the server. Security is guaranteed as long as at most one of the two parties is corrupted. Privacy, on the other hand, guarantees that a corrupted server cannot link a transaction to a particular user. We formally model the notion of PADW schemes and we give an instantiation from blind Schnorr. Our construction supports deterministic key derivation, a feature that is widely used in practice by existing wallet schemes, and it does not rely on any heavy cryptographic primitives. In the full version of this work, we prove our scheme secure against adaptive adversaries in the random oracle model and under standard assumptions. That is, our proof only relies on the assumption that the Schnorr signature scheme is unforgeable and that a public key encryption scheme is CCA-secure.},
author = {Das, Poulami and Erwig, Andreas and Faust, Sebastian},
doi = {10.1007/978-3-031-71073-5_16},
isbn = {9783031710728},
issn = {16113349},
pages = {338--359},
title = {{Shared-Custodial Password-Authenticated Deterministic Wallets}},
url = {https://link.springer.com/10.1007/978-3-031-71073-5_16},
year = {2024}
}
@article{Blocki2015,
abstract = {We report on a user study that provides evidence that spaced repetition and a specific mnemonic technique enable users to successfully recall multiple strong passwords over time. Remote research participants were asked to memorize 4 Person-Action-Object (PAO) stories where they chose a famous person from a drop-down list and were given machine-generated random action-object pairs. Users were also shown a photo of a scene and asked to imagine the PAO story taking place in the scene (e.g., Bill Gates-swallowing-bike on a beach). Subsequently, they were asked to recall the action-object pairs when prompted with the associated scene-person pairs following a spaced repetition schedule over a period of 127+ days. While we evaluated several spaced repetition schedules, the best results were obtained when users initially returned after 12 hours and then in 1.5× increasing intervals: 77% of the participants successfully recalled all 4 stories in 10 tests over a period of ≈ 158 days. Much of the forgetting happened in the first test period (12 hours): 89% of participants who remembered their stories during the first test period successfully remembered them in every subsequent round. These findings, coupled with recent results on naturally rehearsing password schemes, suggest that 4 PAO stories could be used to create usable and strong passwords for 14 sensitive accounts following this spaced repetition schedule, possibly with a few extra upfront rehearsals. In addition, we find statistically significant evidence that with 8 tests over 64 days users who were asked to memorize 4 PAO stories outperform users who are given 4 random action-object pairs, but with 9 tests over 128 days the advantage is not significant. Furthermore, there is an interference effect across multiple PAO stories: the recall rate of 100% (resp. 90%) for participants who were asked to memorize 1 PAO story (resp. 2 PAO stories) is significantly better than the rate for participants who were asked to memorize 4 PAO stories. These findings yield concrete advice for improving constructions of password management schemes and future user studies.},
archivePrefix = {arXiv},
arxivId = {1410.1490},
author = {Blocki, Jeremiah and Komanduri, Saranga and Cranor, Lorrie and Datta, Anupam},
doi = {10.14722/ndss.2015.23094},
eprint = {1410.1490},
journal = {22nd Annual Network and Distributed System Security Symposium, NDSS 2015},
publisher = {The Internet Society},
title = {{Spaced Repetition and Mnemonics Enable Recall of Multiple Strong Passwords}},
year = {2015}
}
@article{Bonneau2012,
abstract = {I'm not proposing any protocols here, I'm talking about passwords, which is what I've spent the last year or so doing now. An interesting problem, which came up in my thesis, is how to tell how strong an individual password is. There's a growing body of publications on how to assess the strength of a big pile of passwords. So if a bunch of passwords leak from a new website there are some measures that I've developed, and some things other people have worked on, to try and compare this new body of passwords to all of the passwords at a different website. But the world of analysing a single password is still in the dark ages I would say. Obviously the difference is that with a group of passwords you can start to do statistics, and you can look at how many passwords are repeated within that set, whereas if you just have one password you have to reason about what set it came from. {\textcopyright} 2012 Springer-Verlag.},
author = {Bonneau, Joseph},
doi = {10.1007/978-3-642-35694-0_11},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonneau - 2012 - Statistical metrics for individual password strength (Transcript of discussion).pdf:pdf},
isbn = {9783642356933},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {87--95},
title = {{Statistical metrics for individual password strength (Transcript of discussion)}},
volume = {7622 LNCS},
year = {2012}
}
@article{Vasek2017,
abstract = {In the cryptocurrency Bitcoin, users can deterministically derive the private keys used for transmitting money from a password. Such “brain wallets” are appealing because they free users from storing their private keys on untrusted computers. Unfortunately, they also enable attackers to conduct unlimited offline password guessing. In this paper, we report on the first large-scale measurement of the use of brain wallets in Bitcoin. Using a wide range of word lists, we evaluated around 300 billion passwords. Surprisingly, after excluding activities by researchers, we identified just 884 brain wallets worth around $100K in use from September 2011 to August 2015. We find that all but 21 wallets were drained, usually within 24 h but often within minutes. We find that around a dozen “drainers” are competing to liquidate brain wallets as soon as they are funded. We find no evidence that users of brain wallets loaded with more bitcoin select stronger passwords, but we do find that brain wallets with weaker passwords are cracked more quickly.},
author = {Vasek, Marie and Bonneau, Joseph and Castellucci, Ryan and Keith, Cameron and Moore, Tyler},
doi = {10.1007/978-3-662-54970-4_36},
isbn = {9783662549698},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bitcoin,Brain wallets,Cybercrime measurement,Passwords},
pages = {609--618},
publisher = {Springer, Berlin, Heidelberg},
title = {{The bitcoin brain drain: Examining the use and abuse of bitcoin brain wallets}},
url = {https://link.springer.com/chapter/10.1007/978-3-662-54970-4_36},
volume = {9603 LNCS},
year = {2017}
}
@article{Das2021,
abstract = {In many cryptocurrencies, the problem of key management has become one of the most fundamental security challenges. Typically, keys are kept in designated schemes called wallets, whose main purpose is to store these keys securely. One such system is the BIP32 wallet (Bitcoin Improvement Proposal 32), which since its introduction in 2012 has been adopted by countless Bitcoin users and is one of the most frequently used wallet system today. Surprisingly, very little is known about the concrete security properties offered by this system. In this work, we propose the first formal analysis of the BIP32 system in its entirety and without any modification. Building on the recent work of Das et al. (CCS '19), we put forth a formal model for hierarchical deterministic wallet systems (such as BIP32) and give a security reduction in this model from the existential unforgeability of the ECDSA signature algorithm that is used in BIP32. We conclude by giving concrete security parameter estimates achieved by the BIP32 standard, and show that by moving to an alternative key derivation method we can achieve a tighter reduction offering an additional 20 bits of security (111 vs. 91 bits of security) at no additional costs.},
author = {Das, Poulami and Erwig, Andreas and Faust, Sebastian and Loss, Julian and Riahi, Siavash},
doi = {10.1145/3460120.3484807},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Das et al. - 2021 - The Exact Security of BIP32 Wallets(2).pdf:pdf},
isbn = {9781450384544},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {BIP32,cryptocurrencies,foundations,wallets},
month = {nov},
pages = {1020--1042},
publisher = {Association for Computing Machinery},
title = {{The Exact Security of BIP32 Wallets}},
year = {2021}
}
@inproceedings{6234435,
abstract = {We report on the largest corpus of user-chosen passwords ever studied, consisting of anonymized password histograms representing almost 70 million Yahoo! users, mitigating privacy concerns while enabling analysis of dozens of subpopulations based on demographic factors and site usage characteristics. This large data set motivates a thorough statistical treatment of estimating guessing difficulty by sampling from a secret distribution. In place of previously used metrics such as Shannon entropy and guessing entropy, which cannot be estimated with any realistically sized sample, we develop partial guessing metrics including a new variant of guesswork parameterized by an attacker's desired success rate. Our new metric is comparatively easy to approximate and directly relevant for security engineering. By comparing password distributions with a uniform distribution which would provide equivalent security against different forms of guessing attack, we estimate that passwords provide fewer than 10 bits of security against an online, trawling attack, and only about 20 bits of security against an optimal offline dictionary attack. We find surprisingly little variation in guessing difficulty; every identifiable group of users generated a comparably weak password distribution. Security motivations such as the registration of a payment card have no greater impact than demographic factors such as age and nationality. Even proactive efforts to nudge users towards better password choices with graphical feedback make little difference. More surprisingly, even seemingly distant language communities choose the same weak passwords and an attacker never gains more than a factor of 2 efficiency gain by switching from the globally optimal dictionary to a population-specific lists. {\textcopyright} 2012 IEEE.},
author = {Bonneau, Joseph},
booktitle = {Proceedings - IEEE Symposium on Security and Privacy},
doi = {10.1109/SP.2012.49},
isbn = {9780769546810},
issn = {10816011},
keywords = {authentication,computer security,data mining,information theory,statistics},
pages = {538--552},
title = {{The science of guessing: Analyzing an anonymized corpus of 70 million passwords}},
year = {2012}
}
@article{Sasse2001,
abstract = {The security research community has recently recognised that user behaviour plays a part in many security failures, and it has become common to refer to users as the 'weakest link in the security, chain'. We argue that simply blaming users will not lead to more effective security systems. Security designers must identify the causes of undesirable user behaviour, and address these to design effective security systems. We present examples of how undesirable user behaviour with passwords can be caused by failure to recognise the characteristics of human memory, unattainable or conflicting task demands, and lack of support, training and motivation. We conclude that existing human/computer interaction knowledge and techniques can be used to prevent or address these problems, and outline a vision of a holistic design approach for usable and effective security.},
author = {Sasse, M. A. and Brostoff, S. and Weirich, D.},
doi = {10.1023/A:1011902718709},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sasse, Brostoff, Weirich - 2001 - Transforming the 'weakest link' - A humancomputer interaction approach to usable and effective securit.pdf:pdf},
issn = {13583948},
journal = {BT Technology Journal},
month = {jul},
number = {3},
pages = {122--131},
title = {{Transforming the 'weakest link' - A human/computer interaction approach to usable and effective security}},
volume = {19},
year = {2001}
}
@article{Adams1999,
author = {Adams, Anne and Sasse, Martina Angela},
doi = {10.1145/322796.322806},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adams, Sasse - 1999 - Users Are Not The Enemy(2).pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
month = {dec},
number = {12},
pages = {40--46},
title = {{Users Are Not The Enemy}},
volume = {42},
year = {1999}
}

@article{McNaughton1991,
abstract = {Behavioral and neurophysiological evidence strongly suggests that, within curtain limits, rodents and humans can keep track of their directional heading relative to an inertial, and hence allocentric coordinate system. This "sense of direction" appears to involve the integration of angular velocity signals that arise primarily in the vestibular system. A hypothesis is proposed in which the integration process, an operation that may be difficult for neurons to implement, is replaced by a linear associative mapping, an operation that is at least theoretically easy to implement with neurons. The proposed system makes use of a set of linearly independent vectors representing the combination of the current head direction, and head angular velocity representations to "recall" the resulting head direction. It is then proposed that visual landmarks become incorporated into the directional system, enabling both the correction of cumulative error and, ultimately, the computation of novel, optimal trajectories between locations. According to the hypothesis, this occurs through the association of hippocampal "local-view" cells (i.e., direction selective "place cells") with "head-direction" cells located downstream in the dorsal presubiculum. The possible neurophysiological and neuroanatomical bases for the proposed system are discussed.},
author = {McNaughton, B. L. and Chen, L. L. and Markus, E. J.},
doi = {10.1162/JOCN.1991.3.2.190},
issn = {0898929X},
journal = {Journal of Cognitive Neuroscience},
number = {2},
publisher = {MIT Press Journals},
title = {{"Dead reckoning," landmark learning, and the sense of direction: A neurophysiological and computational hypothesis}},
volume = {3},
year = {1991}
}
@article{Benhamou1995,
abstract = {This paper reviews spatial memory processes in three highly evolved taxa: hymenoptera, birds and mammals. In these three taxa, the goal location can be memorized egocentrically as a vector specifying the head-referred direction and the distance to the goal, and/or exocentrically as a view specifying the spatial layout of the surrounding landmarks perceived by the animal when standing at the goal. The egocentric coding process requires a path-integration mechanism to update the memorized goal location as a function of the animal's current position. Changes of direction are estimated allothetically (by reference to an external compass) in hymenoptera, idiothetically (on the basis of internal movement-related information) in mammals, and probably in both ways in birds. Computer simulations have shown that path-integration is very sensitive to random errors occurring in idiothetic but not in allothetic estimations. When using the exocentric coding process, hymenoptera store the bearings and angular sizes of landmarks in a compass-oriented colour snapshot taken at the goal. They may then return to the goal by moving so as to reduce the discrepancy between the current view of landmarks and the memorized snapshot. In mammals, this process can be accounted for by a neurobiologically plausible model which highlights the fundamental role of exploration of the environment. The way this process is implemented in birds is less clear. {\textcopyright} 1995.},
author = {Benhamou, Simon and Poucet, Bruno},
doi = {10.1016/0376-6357(95)00060-7},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benhamou, Poucet - 1995 - A comparative analysis of spatial memory processes.pdf:pdf},
issn = {0376-6357},
journal = {Behavioural Processes},
keywords = {Bird,Hymenoptera,Mammal,Model,Spatial memory},
month = {dec},
number = {1-3},
pages = {113--126},
publisher = {Elsevier},
title = {{A comparative analysis of spatial memory processes}},
url = {https://www.sciencedirect.com/science/article/pii/0376635795000607},
volume = {35},
year = {1995}
}
@article{Segen2021,
abstract = {Successful navigation requires memorising and recognising the locations of objects across different perspectives. Although these abilities rely on hippocampal functioning, which is susceptible to degeneration in older adults, little is known about the effects of ageing on encoding and response strategies that are used to recognise spatial configurations. To investigate this, we asked young and older participants to encode the locations of objects in a virtual room shown as a picture on a computer screen. Participants were then shown a second picture of the same room taken from the same (0°) or a different perspective (45° or 135°) and had to judge whether the objects occupied the same or different locations. Overall, older adults had greater difficulty with the task than younger adults although the introduction of a perspective shift between encoding and testing impaired performance in both age groups. Diffusion modelling revealed that older adults adopted a more conservative response strategy, while the analysis of gaze patterns showed an age-related shift in visual-encoding strategies with older adults attending to more information when memorising the positions of objects in space. Overall, results suggest that ageing is associated with declines in spatial processing abilities, with older individuals shifting towards a more conservative decision style and relying more on encoding target object positions using room-based cues compared to younger adults, who focus more on encoding the spatial relationships among object clusters.},
author = {Segen, Vladislava and Avraamides, Marios N. and Slattery, Timothy J. and Wiener, Jan M.},
doi = {10.3758/S13421-020-01089-3/METRICS},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Segen et al. - 2021 - Age-related differences in visual encoding and response strategies contribute to spatial memory deficits.pdf:pdf},
issn = {15325946},
journal = {Memory and Cognition},
keywords = {Aging,Decision making,Eye movements,Perception,Spatial cognition},
month = {feb},
number = {2},
pages = {249--264},
pmid = {32869141},
publisher = {Springer},
title = {{Age-related differences in visual encoding and response strategies contribute to spatial memory deficits}},
url = {https://link.springer.com/article/10.3758/s13421-020-01089-3},
volume = {49},
year = {2021}
}
@article{Bauer1993,
abstract = {We report an experimental study on the effects of diagrams on deductive reasoning with double disjunctions, for example: Raphael is in Tacoma or Julia is in Atlanta, or both. Julia is in Atlanta or Paul is in Philadelphia, or both. What follows? We confirmed that subjects find it difficult to deduce a valid conclusion, such as Julia is in Atlanta, or both Raphael is in Tacoma and Paul is in Philadelphia. In a preliminary study, the format of the premises was either verbal or diagrammatic, and the diagrams used icons to distinguish between inclusive and exclusive disjunctions. The diagrams had no effect on performance. In the main experiment, the diagrams made the alternative possibilities more explicit. The subjects responded faster (about 35 s) and drew many more valid conclusions (nearly 30%) from the diagrams than from the verbal premises. These results corroborate the theory of mental models and have implications for the role of diagrams in reasoning. {\textcopyright} 1993, Association for Psychological Science. All rights reserved.},
author = {Bauer, Malcolm I. and Johnson-Laird, P. N.},
doi = {10.1111/j.1467-9280.1993.tb00584.x},
issn = {14679280},
journal = {Psychological Science},
number = {6},
pages = {372--378},
title = {{How Diagrams Can Improve Reasoning}},
volume = {4},
year = {1993}
}
@article{Cartwright1982,
abstract = {Bees trained to forage at a place specified by landmarks do not construct a cartesian map of the arrangement of landmarks and food source. Instead they store something like a two-dimensional snapshot of their surroundings taken from the food source. To return there, bees move so as to reduce discrepancies between the snapshot and their current retinal image. A computational model embodying these principles mimics the bees' behaviour. {\textcopyright} 1982 Nature Publishing Group.},
author = {Cartwright, B. A. and Collett, T. S.},
doi = {10.1038/295560A0},
issn = {00280836},
journal = {Nature},
number = {5850},
pages = {560--564},
title = {{How honey bees use landmarks to guide their return to a food source}},
volume = {295},
year = {1982}
}
@article{Bucciarelli2007,
abstract = {In this paper, I present a framework where possible relations between learning and mental models are explored. In particular, I'll be concerned with non-symbolic gestures accompanying discourse and their role in inducing the construction of models and therefore deep comprehension and learning in the listener. Also, I'll be concerned with cognitive and socio-cognitive conflicts and their roles in inducing construction of alternative models of a problem and therefore in learning to reason. Human ability to learn is of great importance for individuals interested in change. Indeed, to learn both declarative and procedural knowledge means to change, and in order to be able to intervene on change in a desired way it is necessary to have a theory of the mental representations and processes involved in learning and a theory of the communication and contexts that favour learning. {\textcopyright} Fondazione Rosselli 2007.},
author = {Bucciarelli, Monica},
doi = {10.1007/s11299-006-0026-y},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bucciarelli - 2007 - How the construction of mental models improves learning.pdf:pdf},
issn = {15937879},
journal = {Mind and Society},
keywords = {Cognitive conflict,Gestures,Learning,Mental models},
month = {jun},
number = {1},
pages = {67--89},
title = {{How the construction of mental models improves learning}},
volume = {6},
year = {2007}
}
@article{Allen2006,
abstract = {The episodic buffer component of working memory is assumed to play a role in the binding of features into chunks. A series of experiments compared memory for arrays of colors or shapes with memory for bound combinations of these features. Demanding concurrent verbal tasks were used to investigate the role of general attentional processes, producing load effects that were no greater on memory for feature combinations than for the features themselves. However, the binding condition was significantly less accurate with sequential rather than simultaneous presentation, especially for items earlier in the sequence. The findings are interpreted as evidence of a relatively automatic but fragile visual feature binding mechanism in working memory. Implications for the concept of an episodic buffer are discussed. Copyright 2006 by the American Psychological Association.},
author = {Allen, Richard J. and Baddeley, Alan D. and Hitch, Graham J.},
doi = {10.1037/0096-3445.135.2.298},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
keywords = {Attention,Central executive,Episodic buffer,Feature binding,Working memory},
month = {may},
number = {2},
pages = {298--313},
pmid = {16719655},
title = {{Is the binding of visual features in working memory resource-demanding?}},
volume = {135},
year = {2006}
}
@article{Pals2018,
abstract = {How can science teachers support students in developing an appropriate declarative knowledge base for solving problems? This article focuses on the question whether the development of students' memory of scientific propositions is better served by writing propositions down on paper or by making drawings of propositions either by silent or muttering rehearsal. By means of a memorisation experiment with eighth- and ninth-grade students, we answer this question. In this experiment, students received instruction to memorise nine science propositions and to reproduce them afterwards. To support memorisation students were randomly assigned either to a group that received instruction to write each proposition on paper or to a group that received instruction to make a drawing about the content of the proposition. In addition, half of the students in both groups received instruction to mutter and the other half of them received instruction to write or draw in silence. The main conclusion from the experiment is that after four weeks students who had made a drawing remembered significantly more propositions than those who had memorised the propositions by writing them down. Our research further revealed that it did not matter whether students muttered or memorised silently.},
author = {Pals, Frits F.B. and Tolboom, Jos L.J. and Suhre, Cor J.M. and van Geert, Paul L.C.},
doi = {10.1080/09500693.2017.1407885;PAGE:STRING:ARTICLE/CHAPTER},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pals et al. - 2018 - Memorisation methods in science education tactics to improve the teaching and learning practice.pdf:pdf},
issn = {14645289},
journal = {International Journal of Science Education},
keywords = {Science education,communication,declarative knowledge,imagery,memory strategies,visual representation},
month = {jan},
number = {2},
pages = {227--241},
publisher = {Routledge},
title = {{Memorisation methods in science education: tactics to improve the teaching and learning practice}},
url = {https://www.tandfonline.com/doi/pdf/10.1080/09500693.2017.1407885},
volume = {40},
year = {2018}
}
@article{Awh1998,
abstract = {This article reports 3 experiments that tested a hypothesis regarding the nature of rehearsal in spatial working memory, one in which discrete shifts of spatial selective attention mediate the maintenance of location-specific representations. Experiment 1 demonstrated increases in visual processing efficiency for locations held in working memory, which suggested that attention was oriented toward these locations. Experiment 2 eliminated key alternative explanations for Experiment 1 by using an identical stimulus display with a nonspatial memory task, and little or no facilitation of processing at memorized locations was found under these conditions. Finally, Experiment 3 showed that spatial working memory was impaired when participants were hindered in their ability to attend to memorized locations. It is argued that these results implicate selective spatial attention as a rehearsal mechanism for spatial working memory.},
author = {Awh, Edward and Jonides, John and Reuter-Lorenz, Patricia A.},
doi = {10.1037/0096-1523.24.3.780},
issn = {00961523},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
number = {3},
pages = {780--790},
pmid = {9627416},
publisher = {American Psychological Association Inc.},
title = {{Rehearsal in Spatial Working Memory}},
volume = {24},
year = {1998}
}
@article{Konkle2010,
abstract = {Observers can store thousands of object images in visual long-term memory with high fidelity, but the fidelity of scene representations in long-term memory is not known. Here, we probed scene-representation fidelity by varying the number of studied exemplars in different scene categories and testing memory using exemplar-level foils. Observers viewed thousands of scenes over 5.5 hr and then completed a series of forced-choice tests. Memory performance was high, even with up to 64 scenes from the same category in memory. Moreover, there was only a 2% decrease in accuracy for each doubling of the number of studied scene exemplars. Surprisingly, this degree of categorical interference was similar to the degree previously demonstrated for object memory. Thus, although scenes have often been defined as a superset of objects, our results suggest that scenes and objects may be entities at a similar level of abstraction in visual long-term memory. {\textcopyright} The Author(s) 2010.},
author = {Konkle, Talia and Brady, Timothy F. and Alvarez, George A. and Oliva, Aude},
doi = {10.1177/0956797610385359},
issn = {09567976},
journal = {Psychological Science},
keywords = {memory capacity,object categories,scene categories,visual memory},
month = {nov},
number = {11},
pages = {1551--1556},
pmid = {20921574},
title = {{Scene memory is more detailed than you think: The role of categories in visual long-term memory}},
volume = {21},
year = {2010}
}
@article{Daniel2003,
abstract = {Route directions to reach a target point on a campus were collected from undergraduates. A "good" description and a "poor" one were selected, based on ratings provided by judges in terms of their value for navigational assistance. People unfamiliar with the campus were then required to navigate to the target after studying one of these descriptions. In addition, a "skeletal" description, which contained the essentials needed for navigating, was constructed and used in the experiment. During navigation, we measured the frequencies of stops and of directional errors (whether these errors were self-corrected or corrected by the experimenter). Overall, the good and the skeletal descriptions resulted in better performance than the poor one. Their value as navigational aids was confirmed by measuring the navigation times. Analyzing the structure and content of the descriptions confirmed that the effectiveness of route directions depends on their ability to connect actions to landmarks, that is, to closely link the prescriptive and the descriptive parts of this specific type of spatial discourse. {\textcopyright} 2003, Lawrence Erlbaum Associates, Inc.},
author = {Daniel, Marie Paule and Tom, Ariane and Manghi, Elsa and Denis, Michel},
doi = {10.1207/s15427633scc0304_2},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Daniel et al. - 2003 - Testing the value of route directions through navigational performance.pdf:pdf},
issn = {13875868},
journal = {Spatial Cognition and Computation},
keywords = {Navigational performance,Route directions,Spatial cognition,Wayfinding},
number = {4},
pages = {269--289},
title = {{Testing the value of route directions through navigational performance}},
volume = {3},
year = {2003}
}
@article{Pazzaglia2007,
abstract = {The paper investigates the specific roles of visual-spatial working memory (VSWM) and verbal working memory (VWM) in encoding and retrieval of information conveyed by spatial and nonspatial texts. In two experiments, a total of 109 undergraduate students-54 in Experiment 1, 55 in Experiment 2-listened to spatial and nonspatial texts while performing a spatial (Experiment 1) or verbal (Experiment 2) concurrent task during either encoding or retrieval. Text memorisation and comprehension were tested by free-recall and sentence-verification tasks. The results show that a concurrent spatial task is detrimental to memory performance for spatial text more than for nonspatial text. In contrast, a concurrent verbal task is equally damaging to memory performance for both spatial and nonspatial texts. Moreover, a spatial task interferes with both encoding and retrieval, in contrast with a verbal task, where the interference effect is active only when the task is performed during encoding. Overall, these findings show the involvement of VSWM in the construction and reactivation of mental models derived from spatial descriptions, and the role played by VWM in construction, but not reactivation, of mental models derived from spatial and nonspatial texts. {\textcopyright} Springer-Verlag 2007.},
author = {Pazzaglia, Francesca and {De Beni}, Rossana and Meneghetti, Chiara},
doi = {10.1007/S00426-006-0045-7/FIGURES/2},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pazzaglia, De Beni, Meneghetti - 2007 - The effects of verbal and spatial interference in the encoding and retrieval of spatial and nons.pdf:pdf},
issn = {03400727},
journal = {Psychological Research},
keywords = {Psychology Research},
month = {jul},
number = {4},
pages = {484--494},
pmid = {16482463},
publisher = {Springer},
title = {{The effects of verbal and spatial interference in the encoding and retrieval of spatial and nonspatial texts}},
url = {https://link.springer.com/article/10.1007/s00426-006-0045-7},
volume = {71},
year = {2007}
}
@article{Bainbridge2013,
abstract = {The faces we encounter throughout our lives make different impressions on us: Some are remembered at first glance, while others are forgotten. Previous work has found that the distinctiveness of a face influences its memorability-the degree to which face images are remembered or forgotten. Here, we generalize the concept of face memorability in a large-scale memory study. First, we find that memorability is an intrinsic feature of a face photograph-across observers some faces are consistently more remembered or forgotten than others-indicating that memorability can be used for measuring, predicting, and manipulating subsequent memories. Second, we determine the role that 20 personality, social, and memory-related traits play in face memorability. Whereas we find that certain traits (such as kindness, atypicality, and trustworthiness) contribute to face memorability, they do not suffice to explain the variance in memorability scores, even when accounting for noise and differences in subjective experience. This suggests that memorability itself is a consistent, singular measure of a face that cannot be reduced to a simple combination of personality and social facial attributes. We outline modern neuroscience questions that can be explored through the lens of memorability. {\textcopyright} 2013 American Psychological Association.},
author = {Bainbridge, Wilma A. and Isola, Phillip and Oliva, Aude},
doi = {10.1037/a0033872},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
keywords = {Face photograph database,Facial attributes,Memorability,Memory,Perception},
number = {4},
pages = {1323--1334},
pmid = {24246059},
publisher = {American Psychological Association Inc.},
title = {{The intrinsic memorability of face photographs}},
volume = {142},
year = {2013}
}
@article{Christou2000,
abstract = {Computer generated virtual environments have reached a level of sophistication and ease of production that they are readily available for use in the average psychology laboratory. The potential benefits include cue control, incorporation of interactivity and novelty of environments used. The draw-backs include limitations in realism and lack of fidelity. In this chapter we describe our use of virtual environments to study how 3D space is encoded in humans with special emphasis on realism and interactivity. We describe the computational methods used to implement this realism and give examples from studies concerning spatial memory for object form, spatial layout and scene recognition.},
author = {Christou, Chris and B{\"{u}}lthoff, Heinrich H.},
doi = {10.1007/3-540-45460-8_23},
pages = {317--332},
title = {{Using Realistic Virtual Environments in the Study of Spatial Encoding}},
year = {2000}
}
@article{Coutrot2019,
abstract = {Virtual reality environments presented on tablets and smartphones have potential to aid the early diagnosis of conditions such as Alzheimer's dementia by quantifying impairments in navigation performance. However, it is unclear whether performance on mobile devices can predict navigation errors in the real world. We compared the performance of 49 participants (25 females, 18-35 years old) at wayfinding and path integration tasks designed in our mobile app ‘Sea Hero Quest' with their performance at similar tasks in a real-world environment. We first performed this experiment in the streets of London (UK) and replicated it in Paris (France). In both cities, we found a significant correlation between virtual and real-world wayfinding performance and a male advantage in both environments, although smaller in the real world (Cohen's d in the game = 0.89, in the real world = 0.59). Results in London and Paris were highly similar, and controlling for familiarity with video games did not change the results. The strength of the correlation between real world and virtual environment increased with the difficulty of the virtual wayfinding task, indicating that Sea Hero Quest does not merely capture video gaming skills. The fact that the Sea Hero Quest wayfinding task has real-world ecological validity constitutes a step toward controllable, sensitive, safe, low-cost, and easy to administer digital cognitive assessment of navigation ability.},
author = {Coutrot, Antoine and Schmidt, Sophie and Coutrot, Lena and Pittman, Jessica and Hong, Lynn and Wiener, Jan M. and H{\"{o}}lscher, Christoph and Dalton, Ruth C. and Hornberger, Michael and Spiers, Hugo J.},
doi = {10.1371/JOURNAL.PONE.0213272;},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Coutrot et al. - 2019 - Virtual navigation tested on a mobile app is predictive of real-world wayfinding navigation performance.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
month = {mar},
number = {3},
pmid = {30883560},
publisher = {Public Library of Science},
title = {{Virtual navigation tested on a mobile app is predictive of real-world wayfinding navigation performance}},
volume = {14},
year = {2019}
}
@article{Lukavsky2017,
abstract = {Previous studies have demonstrated that humans have a remarkable capacity to memorise a large number of scenes. The research on memorability has shown that memory performance can be predicted by the content of an image. We explored how remembering an image is affected by the image properties within the context of the reference set, including the extent to which it is different from its neighbours (image-space sparseness) and if it belongs to the same category as its neighbours (uniformity). We used a reference set of 2,048 scenes (64 categories), evaluated pairwise scene similarity using deep features from a pretrained convolutional neural network (CNN), and calculated the image-space sparseness and uniformity for each image. We ran three memory experiments, varying the memory workload with experiment length and colour/greyscale presentation. We measured the sensitivity and criterion value changes as a function of image-space sparseness and uniformity. Across all three experiments, we found separate effects of 1) sparseness on memory sensitivity, and 2) uniformity on the recognition criterion. People better remembered (and correctly rejected) images that were more separated from others. People tended to make more false alarms and fewer miss errors in images from categorically uniform portions of the image-space. We propose that both image-space properties affect human decisions when recognising images. Additionally, we found that colour presentation did not yield better memory performance over grayscale images.},
author = {Lukavsk{\'{y}}, Jiř{\'{i}} and D{\v{e}}cht{\v{e}}renko, Filip},
doi = {10.3758/S13414-017-1375-9/FIGURES/4},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lukavsk{\'{y}}, D{\v{e}}cht{\v{e}}renko - 2017 - Visual properties and memorising scenes Effects of image-space sparseness and uniformity.pdf:pdf},
issn = {1943393X},
journal = {Attention, Perception, and Psychophysics},
keywords = {Categorization,Memory: visual working and short-term memory,Scene perception},
month = {oct},
number = {7},
pages = {2044--2054},
pmid = {28707123},
publisher = {Springer New York LLC},
title = {{Visual properties and memorising scenes: Effects of image-space sparseness and uniformity}},
url = {https://link.springer.com/article/10.3758/s13414-017-1375-9},
volume = {79},
year = {2017}
}
@article{Garden2002,
abstract = {Two experiments employed dual task techniques to explore the role of working memory in route learning and subsequent route retrieval. Experiment 1 involved contrasting performance of two groups of volunteers respectively learning a route from a series of map segments or a series of visually presented nonsense words. Both groups performed learning and recognition under articulatory suppression or concurrent spatial tapping. Both concurrent tasks had an overall disruptive effect on each learning task. However, spatial tapping disrupted route recognition rather more than did articulatory suppression, while the nonsense word recognition was impaired more by articulatory suppression than by concurrent spatial tapping. Experiment 2 again used dual task methodology, but explored route learning by asking volunteers to follow the experimenter through the winding streets of a medieval European town centre. Retrieval involved following the same route while the experimenter followed and noted errors in navigation. Overall the results partially replicated those of Experiment 1 in that both concurrent tasks interfered with route learning. However, volunteers with high spatial ability appeared more affected by the concurrent spatial tapping task, whereas low spatial subjects appeared more affected by the concurrent articulatory suppression task. Results are interpreted to suggest that different aspects of working memory are involved in learning a route from a map with a greater emphasis on visuo-spatial resources, but in tasks set in real environments where many cues of a varied nature are available, only high spatial ability subjects appear to rely heavily upon the visuospatial component of working memory. Copyright {\textcopyright} 2002 John Wiley & Sons, Ltd.},
author = {Garden, Sharin and Cornoldi, Cesare and Logie, Robert H.},
doi = {10.1002/acp.746},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garden, Cornoldi, Logie - 2002 - Visuo-spatial working memory in navigation.pdf:pdf},
issn = {08884080},
journal = {Applied Cognitive Psychology},
month = {jan},
number = {1},
pages = {35--50},
title = {{Visuo-spatial working memory in navigation}},
volume = {16},
year = {2002}
}
@article{Akaygun2014,
abstract = {The features of a concept or principle an individual chooses to highlight in an explanation or description may be related to the medium of communication used. Different aspects of understanding can be revealed through words and through drawings. This two-part exploratory study examined the differences between explanations of physical and chemical equilibria generated by means of words or pictures. Participants included both instructors and students, who were randomly assigned to provide either written explanations or drawings of physical or chemical equilibrium at the macroscopic and particulate levels. For both studies, analyses revealed that significantly different features appeared in the written and pictorial explanations of equilibrium. The written responses focused more on processes such as the dynamic nature of equilibrium, whereas pictorial representations highlighted structural aspects of equilibrium, such as the spatial arrangement of molecules. Regardless of the level of chemistry knowledge, people conveyed the same type of information via the use of different representations and visual tools. {\textcopyright} 2013 {\textcopyright} 2013 Taylor & Francis.},
author = {Akaygun, Sevil and Jones, Loretta L.},
doi = {10.1080/09500693.2013.828361},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Akaygun, Jones - 2014 - Words or Pictures A comparison of written and pictorial explanations of physical and chemical equilibria.pdf:pdf},
issn = {14645289},
journal = {International Journal of Science Education},
keywords = {Chemistry,Drawings,Equilibrium,Explanations,Representations},
number = {5},
pages = {783--807},
publisher = {Routledge},
title = {{Words or Pictures: A comparison of written and pictorial explanations of physical and chemical equilibria}},
volume = {36},
year = {2014}
}

@article{McNamara2003,
abstract = {This chapter summarizes a new theory of spatial memory. According to the theory, when people learn the locations of objects in a new environment, they interpret the spatial structure of that environment in terms of a spatial reference system. Our current conjecture...},
author = {McNamara, Timothy P.},
doi = {10.1007/3-540-45004-1_11},
isbn = {978-3-540-45004-7},
issn = {03029743},
journal = {Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)},
mendeley-groups = {GeoVault/spatialmemory},
pages = {174--191},
publisher = {Springer, Berlin, Heidelberg},
title = {{How Are the Locations of Objects in the Environment Represented in Memory?}},
url = {https://link.springer.com/chapter/10.1007/3-540-45004-1_11},
volume = {2685},
year = {2003}
}

@article{montello1998new,
abstract = {People acquire knowledge about the spatial layout of the places they experience (cities, neighborhoods, buildings). This includes knowledge of locations, distances, and directions. The acquisition of this knowledge begins immediately, as soon as one arrives in a place, but presumably continues over long time periods, for months, years, and even decades. This knowledge can become quite extensive and elaborate. It provides a framework for the organization of experience and supports sophisticated spatial behavior such as creative wayfinding and direction giving. The developmental process of acquiring knowledge about the spatial layout of places over time is termed. Theoreticians from a variety of disciplines (cognitive, developmental, and environmental psychology; behavioral geography; planning and architec:ture; computer science) have attempted to describe and model spatial microgenesis. The long-dominant framework for understanding this process posits the following sequence: knowledge is initially acquired, followed by knowledge, which is followed by knowledge. In brief, according to this, landmark knowledge is knowledge of distinctive objects or scenes stored in memory. Route knowledge is knowledge of travel paths connecting landmarks. Survey knowledge is configurational knowledge of the locations and extents of features in some part of the environment that is not limited to particular travel paths. In this chapter I will reconsider the dominant framework in some detail, identify some problems with it, and offer an alternative framework that I believe is more conceptually coherent and more consistent with research evidence.},
author = {Montello, Daniel R},
doi = {10.1093/oso/9780195103427.003.0011},
journal = {Spatial And Temporal Reasoning In Geographic Information Systems},
mendeley-groups = {GeoVault/spatialmemory},
pages = {143--154},
title = {{A New Framework for Understanding the Acquisition of Spatial Knowledge in Large-Scale Environments}},
year = {2023}
}

@article{Yates2013,
abstract = {First Published in 1999. This title is the third volume in the ten-volume set titled the Selected Works of Frances Yates. Greyscale illustrations and figures are included throughout - alongside the related descriptive work where applicable. The art in this volume seeks to memorise through a technique of impressing 'places' and 'images' on memory. It has usually been classed as 'mnemotechnics', which appears an unimportant branch of human activity. However, the author discusses in this title that the manipulation of images in memory must always, to some extent, involve the psyche.},
author = {Yates, Frances A.},
doi = {10.4324/9781315010960/ART-MEMORY-YATES/RIGHTS-AND-PERMISSIONS},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yates - 2013 - Selected works Volume III Art of memory.pdf:pdf},
isbn = {9781136353611},
journal = {Selected Works: Volume III: Art of Memory},
mendeley-groups = {GeoVault/spatialmemory},
month = {jan},
pages = {1--400},
publisher = {Taylor and Francis},
title = {{Selected works: Volume III: Art of memory}},
url = {https://www.taylorfrancis.com/books/mono/10.4324/9781315010960/art-memory-yates},
volume = {2},
year = {2013}
}

@article{Singh2017,
abstract = {Geocoding is highly prone to error for various reasons. This paper examines the geographical inconsistencies associated with geocoding errors seen when using two freely available geocoding tools, Google Sheets and ggmap. Two hundred restaurants, all recipients of California's Center of Excellence award, were selected for the analysis. The geocoded addresses were plotted on maps using QGIS, Google Maps, OpenStreetMap (OSM), and Google Earth for visualization, comparison, and validation. A stepwise method of analyzing the geographical inconsistencies is provided that can be adapted for any locational analytics. Both Google Sheets and ggmap were able to successfully geocode all 200 addresses, but ggmap incorrectly geocoded eight addresses as being more than 2,000 miles from their actual location. Addresses containing the ampersand character, &, caused ggmap to incorrectly geocode their location. After replacing the ampersand with the word and, ggmap was able to correctly geocode those addresses. The corrected locations plotted on Google Maps and OSM were similar, and they exactly matched the actual locations when plotted on Google Earth. Both Google Sheets and ggmap are equally capable of geocoding physical locations, but R users are advised that addresses for geocoding must be free of the ampersand character if correct results are to be obtained. In addition, geocoded outputs should be plotted on a map using QGIS, ArcGIS, Google Maps, OSM, R, or any other such mapping tools for visualization and validation. This will ensure a high-quality geospatial analysis of places or events when locational information is vital for decision-making.},
author = {Singh, Sushant K.},
doi = {10.1186/s40965-017-0026-3},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh - 2017 - Evaluating two freely available geocoding tools for geographical inconsistencies and geocoding errors.pdf:pdf},
journal = {Open Geospatial Data, Software and Standards},
month = {dec},
number = {1},
publisher = {Springer Science and Business Media LLC},
title = {{Evaluating two freely available geocoding tools for geographical inconsistencies and geocoding errors}},
volume = {2},
year = {2017}
}
@article{Efremova2018,
abstract = {Searching for locations in web data and associating a document with a corresponding place on the map becomes popular in user's daily activities and it is the first step in web page processing. People often manually search for locations on a web page and then use map services to highlight them because geographic information is not always explicitly available. In this work, we present a geo-tagging framework to extract all addresses from web pages. The solution includes an efficient web page processing approach, which combines a probabilistic language model with real-world knowledge of addresses on maps and extends geocoding services from short queries to large text documents and web pages. We discuss the main problems in dealing with web pages such as: web page noise, identification of relevant segments, and extraction of incomplete addresses. The experimental result shows precision above 91 % which outperforms standard baselines.},
author = {Efremova, Julia and Endres, Ian and Vidas, Isaac and Melnik, Ofer},
doi = {10.1007/978-3-319-95786-9_22},
isbn = {9783319957852},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {288--295},
publisher = {Springer Verlag},
title = {{A geo-tagging framework for address extraction from web pages}},
volume = {10933 LNAI},
year = {2018}
}
@article{Leidner2021,
abstract = {While address geocoding has a long-established track record of research as well as industry deployment as part of GIS systems and online Web services, postal addresses are not the only way to textually encode geographic footprints. For instance, toponyms in particular can be recognized in prose text and "toponym resolution" (Leidner, Comput Environ Urban Syst 30(4):400--417, 2006; Toponym resolution in text: annotation, evaluation and applications of spatial grounding of place names. Ph.D. thesis, School of Informatics, University of Edinburgh, Edinburgh, 2007) has been defined as the mapping of a name for a place to a spatial footprint, enables better spatial applications. In this chapter, we review a range of alternative methods to arrive at a geographic footprints, starting out from different types of input such as available unstructured text and structured meta-data such as postal addresses, toponym mentions and geographic phrase resolution as well as KML, GeoRDF and other structured metadata formats. A range of application types are then described that are supported by georeferencing technologies to show the potential inherent in computing with geospatial data at scale.},
author = {Leidner, Jochen L.},
doi = {10.1007/978-3-030-55462-0_16},
isbn = {9783030554620},
journal = {Handbook of Big Geospatial Data},
month = {may},
pages = {429--457},
publisher = {Springer International Publishing},
title = {{A survey of textual data & geospatial technology}},
year = {2021}
}
@article{Zandbergen2008,
abstract = {The widespread availability of powerful geocoding tools in commercial GIS software and the interest in spatial analysis at the individual level have made address geocoding a widely employed technique in many different fields. The most commonly used approach to geocoding employs a street network data model, in which addresses are placed along a street segment based on a linear interpolation of the location of the street number within an address range. Several alternatives have emerged, including the use of address points and parcels, but these have not received widespread attention in the literature. This paper reviews the foundation of geocoding and presents a framework for evaluating geocoding quality based on completeness, positional accuracy and repeatability. Geocoding quality was compared using three address data models: address points, parcels and street networks. The empirical evaluation employed a variety of different address databases for three different Counties in Florida. Results indicate that address point geocoding produces geocoding match rates similar to those observed for street network geocoding. Parcel geocoding generally produces much lower match rates, in particular for commercial and multi-family residential addresses. Variability in geocoding match rates between address databases and between geographic areas is substantial, reinforcing the need to strengthen the development of standards for address reference data and improved address data entry validation procedures. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Zandbergen, Paul A.},
doi = {10.1016/j.compenvurbsys.2007.11.006},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zandbergen - 2008 - A comparison of address point, parcel and street geocoding techniques(2).pdf:pdf},
issn = {01989715},
journal = {Computers, Environment and Urban Systems},
keywords = {Address models,Address points,Geocoding,Parcels,Reference data},
month = {may},
number = {3},
pages = {214--232},
title = {{A comparison of address point, parcel and street geocoding techniques}},
volume = {32},
year = {2008}
}
@article{Jiang2018,
abstract = {With the advent of location-based services, the demand for location data has dramatically increased. Geocoded locations have become necessary in many GIS analysis, cartography, and decision-making workflows. A reliable geocoding system that can effectively return any location on earth with sufficient accuracy is desired. This study is motivated by a need for a geocoding system to support university campus applications. Address-based geocoding systems have been used for decades. However, they present limitations in address resources, address standardization, and address database maintenance. These limitations have recently sparked an interest in developing alternative geocoding systems that apply alphanumeric codes as a reference to locations, such as Geohash, Google's Open Location Code, and what3words to name a few. Comparing to other geocoders, what3words (w3w) has many advantages. It uses a simple format of code consisting of three words, it is less error-prone, codes are easier to memorize, and multiple languages are supported. However, its fixed resolution (consisting of 3 m by 3 m square cells) and lack of consideration of the third dimension may limit its applicability. To better support geographic applications with special requirements, the w3w geocoding system needs to be extended. This paper proposes extensions of w3w in two aspects: variable resolution and third dimension support. A geocoding processing tool that implements these extensions is being developed to support the need of a university campus' facility management, emergency evacuation and route navigation planning, student survey data management, and other location-based services.},
author = {Jiang, Wen and Stefanakis, Emmanuel},
doi = {10.1007/s41651-018-0014-x},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Stefanakis - 2018 - What3Words Geocoding Extensions(2).pdf:pdf},
issn = {25098829},
journal = {Journal of Geovisualization and Spatial Analysis},
keywords = {Geocoding processing,Geocoding systems,Location-based services,what3words},
month = {jun},
number = {1},
pages = {1--18},
publisher = {Springer Nature},
title = {{What3Words Geocoding Extensions}},
url = {https://link.springer.com/article/10.1007/s41651-018-0014-x},
volume = {2},
year = {2018}
}
@article{Lee2009,
abstract = {For more than four decades, two address-matching methods, the street-based address geocoding method and address-point-matching method, have been used to identify geographical coordinates from postal addresses. However, street-based address geocoding methods developed for the US addressing system are not universally applicable in developing a single-portal geocoding middleware for worldwide Internet-based geographic information systems applications. Problems also exist with address-point matching especially in its capability to identify features and incorporate 3D locational data from 3D addresses for analysis of large public buildings, shopping centers or metro-subways, that exist in urban environments. To alleviate these problems, this paper details two alternative address-matching methods, an area-based address geocoding method and a 3D address geocoding method. The area-based address geocoding method is a 2D positioning method based on a 2D area-based address-matching technique. The 3D address geocoding method is based on a universally applicable 3D address-geocoding technique. To elaborate, this paper introduces (1) a BlockObject model and designs reference databases for an area-based address system, (2) a 3D indoor network model representing the internal structures of urban environment, and (3) a 3D address geocoding algorithm based on a 3D indoor geocoding method. To illustrate the benefits of the 3D address-positioning method, this paper implements 3D indoor navigation to define optimal routes within a single building. {\textcopyright} 2008 Pion Ltd and its Licensors.},
author = {Lee, Jiyeong},
doi = {10.1068/b31169},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee - 2009 - GIS-based geocoding methods for area-based addresses and 3D addresses in urban areas.pdf:pdf},
issn = {14723417},
journal = {Environment and Planning B: Planning and Design},
number = {1},
pages = {86--106},
publisher = {Pion Limited},
title = {{GIS-based geocoding methods for area-based addresses and 3D addresses in urban areas}},
volume = {36},
year = {2009}
}
@article{Lee2020,
abstract = {Address matching is a crucial step in geocoding; however, this step forms a bottleneck for geocoding accuracy, as precise input is the biggest challenge for establishing perfect matches. Matches still have to be established despite the inevitability of incorrect address inputs such as misspellings, abbreviations, informal and non-standard names, slangs, or coded terms. Thus, this study suggests an address geocoding system using machine learning to enhance the address matching implemented on street-based addresses. Three different kinds of machine learning methods are tested to find the best method showing the highest accuracy. The performance of address matching using machine learning models is compared to multiple text similarity metrics, which are generally used for the word matching. It was proved that extreme gradient boosting with the optimal hyper-parameters was the best machine learning method with the highest accuracy in the address matching process, and the accuracy of extreme gradient boosting outperformed similarity metrics when using training data or input data. The address matching process using machine learning achieved high accuracy and can be applied to any geocoding systems to precisely convert addresses into geographic coordinates for various research and applications, including car navigation.},
author = {Lee, Kangjae and Claridades, Alexis Richard C. and Lee, Jiyeong},
doi = {10.3390/app10165628},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Claridades, Lee - 2020 - Improving a street-based geocoding algorithm using machine learning techniques.pdf:pdf},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {Address,Alias,Geocoding,Machine learning},
month = {aug},
number = {16},
publisher = {MDPI AG},
title = {{Improving a street-based geocoding algorithm using machine learning techniques}},
volume = {10},
year = {2020}
}
@article{Goldberg2013,
abstract = {Background: Geocoding, the process of converting textual information describing a location into one or more digital geographic representations, is a routine task performed at large organizations and government agencies across the globe. In a health context, this task is often a fundamental first step performed prior to all operations that take place in a spatially-based health study. As such, the quality of the geocoding system used within these agencies is of paramount concern to the agency (the producer) and researchers or policy-makers who wish to use these data (consumers). However, geocoding systems are continually evolving with new products coming on the market continuously. Agencies must develop and use criteria across a number axes when faced with decisions about building, buying, or maintaining any particular geocoding systems. To date, published criteria have focused on one or more aspects of geocode quality without taking a holistic view of a geocoding system's role within a large organization. The primary purpose of this study is to develop and test an evaluation framework to assist a large organization in determining which geocoding systems will meet its operational needs.Methods: A geocoding platform evaluation framework is derived through an examination of prior literature on geocoding accuracy. The framework developed extends commonly used geocoding metrics to take into account the specific concerns of large organizations for which geocoding is a fundamental operational capability tightly-knit into its core mission of processing health data records. A case study is performed to evaluate the strengths and weaknesses of five geocoding platforms currently available in the Australian geospatial marketplace.Results: The evaluation framework developed in this research is proven successful in differentiating between key capabilities of geocoding systems that are important in the context of a large organization with significant investments in geocoding resources. Results from the proposed methodology highlight important differences across all axes of geocoding system comparisons including spatial data output accuracy, reference data coverage, system flexibility, the potential for tight integration, and the need for specialized staff and/or development time and funding. Such results can empower decisions-makers within large organizations as they make decisions and investments in geocoding systems. {\textcopyright} 2013 Goldberg et al.; licensee BioMed Central Ltd.},
author = {Goldberg, Daniel W. and Ballard, Morven and Boyd, James H. and Mullan, Narelle and Garfield, Carol and Rosman, Diana and Ferrante, Anna M. and Semmens, James B.},
doi = {10.1186/1476-072X-12-50},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldberg et al. - 2013 - An evaluation framework for comparing geocoding systems.pdf:pdf},
issn = {1476072X},
journal = {International Journal of Health Geographics},
keywords = {Geocoding,Georeferencing,Health records,Postal address data,Record linkage},
month = {nov},
pmid = {24207169},
title = {{An evaluation framework for comparing geocoding systems}},
volume = {12},
year = {2013}
}
@article{Zandbergen2008a,
abstract = {The widespread availability of powerful geocoding tools in commercial GIS software and the interest in spatial analysis at the individual level have made address geocoding a widely employed technique in many different fields. The most commonly used approach to geocoding employs a street network data model, in which addresses are placed along a street segment based on a linear interpolation of the location of the street number within an address range. Several alternatives have emerged, including the use of address points and parcels, but these have not received widespread attention in the literature. This paper reviews the foundation of geocoding and presents a framework for evaluating geocoding quality based on completeness, positional accuracy and repeatability. Geocoding quality was compared using three address data models: address points, parcels and street networks. The empirical evaluation employed a variety of different address databases for three different Counties in Florida. Results indicate that address point geocoding produces geocoding match rates similar to those observed for street network geocoding. Parcel geocoding generally produces much lower match rates, in particular for commercial and multi-family residential addresses. Variability in geocoding match rates between address databases and between geographic areas is substantial, reinforcing the need to strengthen the development of standards for address reference data and improved address data entry validation procedures. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Zandbergen, Paul A.},
doi = {10.1016/j.compenvurbsys.2007.11.006},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zandbergen - 2008 - A comparison of address point, parcel and street geocoding techniques.pdf:pdf},
issn = {01989715},
journal = {Computers, Environment and Urban Systems},
keywords = {Address models,Address points,Geocoding,Parcels,Reference data},
month = {may},
number = {3},
pages = {214--232},
title = {{A comparison of address point, parcel and street geocoding techniques}},
volume = {32},
year = {2008}
}
@article{Jiang2018a,
abstract = {With the advent of location-based services, the demand for location data has dramatically increased. Geocoded locations have become necessary in many GIS analysis, cartography, and decision-making workflows. A reliable geocoding system that can effectively return any location on earth with sufficient accuracy is desired. This study is motivated by a need for a geocoding system to support university campus applications. Address-based geocoding systems have been used for decades. However, they present limitations in address resources, address standardization, and address database maintenance. These limitations have recently sparked an interest in developing alternative geocoding systems that apply alphanumeric codes as a reference to locations, such as Geohash, Google's Open Location Code, and what3words to name a few. Comparing to other geocoders, what3words (w3w) has many advantages. It uses a simple format of code consisting of three words, it is less error-prone, codes are easier to memorize, and multiple languages are supported. However, its fixed resolution (consisting of 3 m by 3 m square cells) and lack of consideration of the third dimension may limit its applicability. To better support geographic applications with special requirements, the w3w geocoding system needs to be extended. This paper proposes extensions of w3w in two aspects: variable resolution and third dimension support. A geocoding processing tool that implements these extensions is being developed to support the need of a university campus' facility management, emergency evacuation and route navigation planning, student survey data management, and other location-based services.},
author = {Jiang, Wen and Stefanakis, Emmanuel},
doi = {10.1007/s41651-018-0014-x},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Stefanakis - 2018 - What3Words Geocoding Extensions.pdf:pdf},
issn = {25098829},
journal = {Journal of Geovisualization and Spatial Analysis},
keywords = {Geocoding processing,Geocoding systems,Location-based services,what3words},
month = {jun},
number = {1},
publisher = {Springer Nature},
title = {{What3Words Geocoding Extensions}},
volume = {2},
year = {2018}
}
@article{Arthur2023,
abstract = {What3Words is a geocoding application that uses triples of words instead of alphanumeric coordinates to identify locations. What3Words has grown rapidly in popularity over the past few years and is used in logistical applications worldwide, including by emergency services. What3Words has also attracted criticism for being less reliable than claimed, in particular that the chance of confusing one address with another is high. This paper investigates these claims and shows that the What3Words algorithm for assigning addresses to grid boxes creates many pairs of confusable addresses, some of which are quite close together. The implications of this for the use of What3Words in critical or emergency situations is discussed.},
archivePrefix = {arXiv},
arxivId = {2308.16025},
author = {Arthur, Rudy},
doi = {10.1371/journal.pone.0292491},
eprint = {2308.16025},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arthur - 2023 - A critical analysis of the What3Words geocoding algorithm.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Algorithms,Latitude,Longitude,Phonology,Probability distribution,Semantics,Valleys,Verbal communication},
month = {oct},
number = {10 October},
pages = {e0292491},
pmid = {37878572},
publisher = {Public Library of Science},
title = {{A critical analysis of the What3Words geocoding algorithm}},
url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0292491},
volume = {18},
year = {2023}
}
@misc{Mukundan2019,
abstract = {We propose a kernelized deep local-patch descriptor based on efficient match kernels of neural network activations. Response of each receptive field is encoded together with its spatial location using explicit feature maps. Two location parametrizations, Cartesian and polar, are used to provide robustness to a different types of canonical patch misalignment. Additionally, we analyze how the conventional architecture, i.e. a fully connected layer attached after the convolutional part, encodes responses in a spatially variant way. In contrary, explicit spatial encoding is used in our descriptor, whose potential applications are not limited to local-patches. We evaluate the descriptor on standard benchmarks. Both versions, encoding 32x32 or 64x64 patches, consistently outperform all other methods on all benchmarks. The number of parameters of the model is independent of the input patch resolution.},
archivePrefix = {arXiv},
arxivId = {1904.07190},
author = {Mukundan, Arun and Tolias, Giorgos and Chum, Ondrej},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00962},
eprint = {1904.07190},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mukundan, Tolias, Chum - 2019 - Explicit Spatial Encoding for Deep Local Descriptors.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
keywords = {Categorization,Recognition: Detection,Representation Learning,Retrieval},
pages = {9386--9395},
title = {{Explicit spatial encoding for deep local descriptors}},
volume = {2019-June},
year = {2019}
}
@article{Mai2022,
abstract = {A common need for artificial intelligence models in the broader geoscience is to encode various types of spatial data, such as points, polylines, polygons, graphs, or rasters, in a hidden embedding space so that they can be readily incorporated into deep learning models. One fundamental step is to encode a single point location into an embedding space, such that this embedding is learning-friendly for downstream machine learning models. We call this process location encoding. However, there lacks a systematic review on location encoding, its potential applications, and key challenges that need to be addressed. This paper aims to fill this gap. We first provide a formal definition of location encoding, and discuss the necessity of it for GeoAI research. Next, we provide a comprehensive survey about the current landscape of location encoding research. We classify location encoding models into different categories based on their inputs and encoding methods, and compare them based on whether they are parametric, multi-scale, distance preserving, and direction aware. We demonstrate that existing location encoders can be unified under one formulation framework. We also discuss the application of location encoding. Finally, we point out several challenges that need to be solved in the future.},
archivePrefix = {arXiv},
arxivId = {2111.04006},
author = {Mai, Gengchen and Janowicz, Krzysztof and Hu, Yingjie and Gao, Song and Yan, Bo and Zhu, Rui and Cai, Ling and Lao, Ni},
doi = {10.1080/13658816.2021.2004602},
eprint = {2111.04006},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mai et al. - 2022 - A review of location encoding for GeoAI methods and applications(2).pdf:pdf},
issn = {13623087},
journal = {International Journal of Geographical Information Science},
keywords = {GeoAI,Location encoding,representation learning,spatially explicit machine learning},
number = {4},
pages = {639--673},
publisher = {Taylor & Francis},
title = {{A review of location encoding for GeoAI: methods and applications}},
url = {https://www.tandfonline.com/doi/pdf/10.1080/13658816.2021.2004602},
volume = {36},
year = {2022}
}
@article{Karimi2011,
abstract = {Today, many services that can geocode addresses are available to domain scientists and researchers, software developers, and end-users. For a number of reasons, including quality of reference database and interpolation technique, a given address geocoded by different services does not often result in the same location. Considering that there are many widely available and accessible geocoding services and that each geocoding service may utilize a different reference database and interpolation technique, selecting a suitable geocoding service that meets the requirements of any application or user is a challenging task. This is especially true for online geocoding services which are often used as black boxes and do not provide knowledge about the reference databases and the interpolation techniques they employ. In this article, we present a geocoding recommender algorithm that can recommend optimal online geocoding services by realizing the characteristics (positional accuracy and match rate) of the services and preferences of the user and/or their application. The algorithm is simulated and analyzed using six popular online geocoding services for different address types (agricultural, commercial, industrial, residential) and preferences (match rate, positional accuracy). {\textcopyright} 2011 Blackwell Publishing Ltd.},
author = {Karimi, Hassan A. and Sharker, Monir H. and Roongpiboonsopit, Duangduen},
doi = {10.1111/j.1467-9671.2011.01293.x},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karimi, Sharker, Roongpiboonsopit - 2011 - Geocoding recommender an algorithm to recommend optimal online geocoding services for applica.pdf:pdf},
issn = {14679671},
journal = {Transactions in GIS},
number = {6},
pages = {869--886},
title = {{Geocoding recommender: An algorithm to recommend optimal online geocoding services for applications}},
volume = {15},
year = {2011}
}

@article{Boneh2018,
abstract = {We study the problem of building a verifiable delay function (VDF). A VDF requires a specified number of sequential steps to evaluate, yet produces a unique output that can be efficiently and publicly verified. VDF s have many applications in decentralized systems, including public randomness beacons, leader election in consensus protocols, and proofs of replication. We formalize the requirements for VDF s and present new candidate constructions that are the first to achieve an exponential gap between evaluation and verification time.},
author = {Boneh, Dan and Bonneau, Joseph and B{\"{u}}nz, Benedikt and Fisch, Ben},
doi = {10.1007/978-3-319-96884-1_25},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boneh et al. - 2018 - Verifiable delay functions.pdf:pdf},
isbn = {9783319968834},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {757--788},
publisher = {Springer Verlag},
title = {{Verifiable delay functions}},
volume = {10991 LNCS},
year = {2018}
}
@article{Jaques2021,
abstract = {Time-release cryptography requires problems that take a long time to solve and take just as long even with significant computational resources. While time-release cryptography originated with the seminal paper of Rivest, Shamir and Wagner ('96), it has gained special visibility recently due to new time-release primitives, like verifiable delay functions (VDFs) and sequential proofs of work, and their novel blockchain applications. In spite of this recent progress, security definitions remain inconsistent and fragile, and foundational treatment of these primitives is scarce. Relationships between the various time-release primitives are elusive, with few connections to standard cryptographic assumptions. We systematically address these drawbacks. We define formal notions of sequential functions, the building blocks of time-release cryptography. The new definitions are robust against change of machine models, making them more amenable to complexity theoretic treatment. We demonstrate the equivalence of various types of sequential functions under standard cryptographic assumptions. The time-release primitives in the literature (such as those defined by Bitansky et al. (ITCS '16)) imply that these primitives exist, as well as the converse. However, showing that a given construction is a sequential function is a hard circuit lower bound problem. To our knowledge, no results show that standard cryptographic assumptions imply any sequentiality. For example, repeated squaring over RSA groups is assumed to be sequential, but nothing connects this conjecture to standard hardness assumptions. To circumvent this, we construct a function that we prove is sequential if there exists any sequential function, without needing any specific knowledge of this hypothetical function. Our techniques use universal circuits and fully homomorphic encryption and generalize some of the elegant techniques of the recent work on lattice NIZKs (Canetti et al., STOC '19). Using our reductions and sequential function constructions, we build VDFs and sequential proofs of work from fully homomorphic encryption, incremental verifiable computation, and the existence of a sequential function. Though our constructions are theoretical in nature and not competitive with existing techniques, they are built from much weaker assumptions than known constructions.},
author = {Jaques, Samuel and Montgomery, Hart and Rosie, Razvan and Roy, Arnab},
doi = {10.1007/978-3-030-92518-5_26},
isbn = {9783030925178},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {584--606},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Time-Release Cryptography from Minimal Circuit Assumptions}},
volume = {13143 LNCS},
year = {2021}
}
@article{Mahmoody2011,
abstract = {A time-lock puzzle is a mechanism for sending messages "to the future". The sender publishes a puzzle whose solution is the message to be sent, thus hiding it until enough time has elapsed for the puzzle to be solved. For time-lock puzzles to be useful, generating a puzzle should take less time than solving it. Since adversaries may have access to many more computers than honest solvers, massively parallel solvers should not be able to produce a solution much faster than serial ones. To date, we know of only one mechanism that is believed to satisfy these properties: the one proposed by Rivest, Shamir and Wagner (1996), who originally introduced the notion of time-lock puzzles. Their puzzle is based on the serial nature of exponentiation and the hardness of factoring, and is therefore vulnerable to advances in factoring techniques (as well as to quantum attacks). In this work, we study the possibility of constructing time-lock puzzles in the random-oracle model. Our main result is negative, ruling out time-lock puzzles that require more parallel time to solve than the total work required to generate a puzzle. In particular, this should rule out black-box constructions of such time-lock puzzles from one-way permutations and collision-resistant hash-functions. On the positive side, we construct a time-lock puzzle with a linear gap in parallel time: a new puzzle can be generated with one round of n parallel queries to the random oracle, but n rounds of serial queries are required to solve it (even for massively parallel adversaries). {\textcopyright} 2011 International Association for Cryptologic Research.},
author = {Mahmoody, Mohammad and Moran, Tal and Vadhan, Salil},
doi = {10.1007/978-3-642-22792-9_3},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahmoody, Moran, Vadhan - 2011 - Time-lock puzzles in the random oracle model.pdf:pdf},
isbn = {9783642227912},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {39--50},
publisher = {Springer Verlag},
title = {{Time-lock puzzles in the random oracle model}},
volume = {6841 LNCS},
year = {2011}
}
@article{Choi2020,
abstract = {Timed-release encryption allows senders to send a message to a receiver which cannot decrypt until a server releases a time bound key at the release time. The release time usually supposed to be known to the receiver, the ciphertext therefore cannot be decrypted if the release time is lost. We solve this problem in this paper by having a master time bound key which can replace the time bound key of any release time. We first present security models of the timed-release encryption with master time bound key. We present a provably secure construction based on the Weil pairing.},
author = {Choi, Gwangbae and Vaudenay, Serge},
doi = {10.1007/978-3-030-39303-8_13},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Vaudenay - 2020 - Timed-Release Encryption with Master Time Bound Key.pdf:pdf},
isbn = {9783030393021},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bilinear Diffie-Hellman problem,Timed-release encryption,Weil pairing},
pages = {167--179},
publisher = {Springer},
title = {{Timed-Release Encryption with Master Time Bound Key}},
volume = {11897 LNCS},
year = {2020}
}
@article{Boneh2000,
abstract = {We introduce and construct timed commitment schemes, an extension to the standard notion of commitments in which a potential forced opening phase permits the receiver to recover (with effort) the committed value without the help of the committer. An important application of our timed-commitment scheme is contract signing: two mutually suspicious parties wish to exchange signatures on a contract. We show a two-party protocol that allows them to exchange RSA or Rabin signatures. The protocol is strongly fair: if one party quits the protocol early, then the two parties must invest comparable amounts of time to retrieve the signatures. This statement holds even if one party has many more machines than the other. Other applications, including honesty preserving auctions and collective coin-flipping, are discussed.},
author = {Boneh, Dan and Naor, Moni},
doi = {10.1007/3-540-44598-6_15},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boneh, Naor - 2000 - Timed commitments(2).pdf:pdf},
isbn = {9783540445982},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {236--254},
publisher = {Springer Verlag},
title = {{Timed commitments}},
volume = {1880},
year = {2000}
}
@article{Dottling2020,
abstract = {A Verifiable Delay Function (VDF) is a function that takes at least T sequential steps to evaluate and produces a unique output that can be verified efficiently, in time essentially independent of T. In this work we study tight VDFs, where the function can be evaluated in time not much more than the sequentiality bound T. On the negative side, we show the impossibility of a black-box construction from random oracles of a VDF that can be evaluated in time $$T + O(T^\delta )$$ for any constant $$\delta < 1$$. On the positive side, we show that any VDF with an inefficient prover (running in time cT for some constant c) that has a natural self-composability property can be generically transformed into a VDF with a tight prover efficiency of $$T+O(1)$$. Our compiler introduces only a logarithmic factor overhead in the proof size and in the number of parallel threads needed by the prover. As a corollary, we obtain a simple construction of a tight VDF from any succinct non-interactive argument combined with repeated hashing. This is in contrast with prior generic constructions (Boneh et al., CRYPTO 2018) that required the existence of incremental verifiable computation, which entails stronger assumptions and complex machinery.},
author = {D{\"{o}}ttling, Nico and Garg, Sanjam and Malavolta, Giulio and Vasudevan, Prashant Nalini},
doi = {10.1007/978-3-030-57990-6_4},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/D{\"{o}}ttling et al. - 2020 - Tight verifiable delay functions.pdf:pdf},
isbn = {9783030579890},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {65--84},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Tight verifiable delay functions}},
volume = {12238 LNCS},
year = {2020}
}
@article{Courtois2016,
abstract = {In this paper, we study and give the first detailed benchmarks on existing implementations of the secp256k1 elliptic curve used by at least hundreds of thousands of users in Bitcoin and other cryptocurrencies. Our implementation improves the state of the art by a factor of 2.5 with a focus on the cases, where side channel attacks are not a concern and a large quantity of RAM is available. As a result, we are able to scan the Bitcoin blockchain for weak keys faster than any previous implementation. We also give some examples of passwords which we have cracked, showing that brain wallets are not secure in practice even for quite complex passwords.},
author = {Courtois, Nicolas and Song, Guangyan and Castellucci, Ryan},
doi = {10.1515/tmmp-2016-0030},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Courtois, Song, Castellucci - 2016 - {\O} {\~{N}} {\AA}{\AA}{\O}{\O}{\O}{\~{N}}{\~{N}}{\O}{\O}{\O}{\O}Ð {\`{E}}{\`{U}}{\`{U}}ÐÐÐÐ{\O}{\O}{\'{O}}{\`{O}}× SPEED OPTIMIZATIONS IN BITCOIN KEY RECOVERY ATTACKS.pdf:pdf},
issn = {12103195},
journal = {Tatra Mountains Mathematical Publications},
keywords = {Bitcoin,Brain wallet,Crypto currency,Elliptic curve cryptography},
number = {1},
pages = {55--68},
title = {{Speed Optimizations in Bitcoin Key Recovery Attacks}},
volume = {67},
year = {2016}
}
@article{Cohen2018,
abstract = {At ITCS 2013, Mahmoody, Moran and Vadhan [MMV13] introduce and construct publicly verifiable proofs of sequential work, which is a protocol for proving that one spent sequential computational work related to some statement. The original motivation for such proofs included non-interactive time-stamping and universally verifiable CPU benchmarks. A more recent application, and our main motivation, are blockchain designs, where proofs of sequential work can be used – in combination with proofs of space – as a more ecological and economical substitute for proofs of work which are currently used to secure Bitcoin and other cryptocurrencies. The construction proposed by [MMV13] is based on a hash function and can be proven secure in the random oracle model, or assuming inherently sequential hash-functions, which is a new standard model assumption introduced in their work. In a proof of sequential work, a prover gets a “statement” $\chi$, a time parameter N and access to a hash-function H, which for the security proof is modelled as a random oracle. Correctness requires that an honest prover can make a verifier accept making only N queries to H, while soundness requires that any prover who makes the verifier accept must have made (almost) N sequential queries to H. Thus a solution constitutes a proof that N time passed since $\chi$ was received. Solutions must be publicly verifiable in time at most polylogarithmic in N. The construction of [MMV13] is based on “depth-robust” graphs, and as a consequence has rather poor concrete parameters. But the major drawback is that the prover needs not just N time, but also N space to compute a proof. In this work we propose a proof of sequential work which is much simpler, more efficient and achieves much better concrete bounds. Most importantly, the space required can be as small as log (N) (but we get better soundness using slightly more memory than that). An open problem stated by [MMV13] that our construction does not solve either is achieving a “unique” proof, where even a cheating prover can only generate a single accepting proof. This property would be extremely useful for applications to blockchains.},
author = {Cohen, Bram and Pietrzak, Krzysztof},
doi = {10.1007/978-3-319-78375-8_15},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Pietrzak - 2018 - Simple proofs of sequential work.pdf:pdf},
isbn = {9783319783741},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {451--467},
publisher = {Springer Verlag},
title = {{Simple proofs of sequential work}},
volume = {10821 LNCS},
year = {2018}
}
@article{Abusalah2019,
abstract = {Proofs of sequential work (PoSW) are proof systems where a prover, upon receiving a statement (formula presented) and a time parameter T computes a proof (formula presented) which is efficiently and publicly verifiable. The proof can be computed in T sequential steps, but not much less, even by a malicious party having large parallelism. A PoSW thus serves as a proof that T units of time have passed since (formula presented) was received. PoSW were introduced by Mahmoody, Moran and Vadhan [MMV11], a simple and practical construction was only recently proposed by Cohen and Pietrzak [CP18]. In this work we construct a new simple PoSW in the random permutation model which is almost as simple and efficient as [CP18] but conceptually very different. Whereas the structure underlying [CP18] is a hash tree, our construction is based on skip lists and has the interesting property that computing the PoSW is a reversible computation. The fact that the construction is reversible can potentially be used for new applications like constructing proofs of replication. We also show how to “embed” the sloth function of Lenstra and Weselowski [LW17] into our PoSW to get a PoSW where one additionally can verify correctness of the output much more efficiently than recomputing it (though recent constructions of “verifiable delay functions” subsume most of the applications this construction was aiming at).},
author = {Abusalah, Hamza and Kamath, Chethan and Klein, Karen and Pietrzak, Krzysztof and Walter, Michael},
doi = {10.1007/978-3-030-17656-3_10},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abusalah et al. - 2019 - Reversible proofs of sequential work.pdf:pdf},
isbn = {9783030176556},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {277--291},
publisher = {Springer Verlag},
title = {{Reversible proofs of sequential work}},
volume = {11477 LNCS},
year = {2019}
}
@article{Cheon2008,
abstract = {A timed-release cryptosystem allows a sender to encrypt a message so that only the intended recipient can read it only after a specified time. We formalize the concept of a secure timed-release public-key cryptosystem and show that, if a third party is relied upon to guarantee decryption after the specified date, this concept is equivalent to identity-based encryption; this explains the observation that all known constructions use identity-based encryption to achieve timed-release security. We then give several provably-secure constructions of timed-release encryption: a generic scheme based on any identity-based encryption scheme, and two more efficient schemes based on the existence of cryptographically admissible bilinear mappings. The first of these is essentially as efficient as the Boneh-Franklin Identity-Based encryption scheme, and is provably secure and authenticated in the random oracle model; the final scheme is not authenticated but is provably secure in the standard model (i.e., without random oracles). {\textcopyright} 2008 ACM.},
author = {Cheon, Jung Hee and Hopper, Nicholas and Kim, Yongdae and Osipkov, Ivan},
doi = {10.1145/1330332.1330336},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheon et al. - 2008 - Provably secure timed-release public key encryption.pdf:pdf},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
keywords = {Authenticated encryption,Key-insulated encryption,Timed-release},
month = {may},
number = {2},
pages = {4},
title = {{Provably secure timed-release public key encryption}},
volume = {11},
year = {2008}
}
@article{Dziembowski2015,
abstract = {Proofs of work (PoW) have been suggested by Dwork and Naor (Crypto'92) as protection to a shared resource. The basic idea is to ask the service requestor to dedicate some non-trivial amount of computational work to every request. The original applications included prevention of spam and protection against denial of service attacks. More recently, PoWs have been used to prevent double spending in the Bitcoin digital currency system. In this work, we put forward an alternative concept for PoWs - so-called proofs of space (PoS), where a service requestor must dedicate a significant amount of disk space as opposed to computation. We construct secure PoS schemes in the random oracle model (with one additional mild assumption required for the proof to go through), using graphs with high “pebbling complexity” and Merkle hash-trees. We discuss some applications, including follow-up work where a decentralized digital currency scheme called Spacecoin is constructed that uses PoS (instead of wasteful PoW like in Bitcoin) to prevent double spending. The main technical contribution of this work is the construction of (directed, loop-free) graphs on N vertices with in-degree O(log logN) such that even if one places $\Theta$(N) pebbles on the nodes of the graph, there's a constant fraction of nodes that needs $\Theta$(N) steps to be pebbled (where in every step one can put a pebble on a node if all its parents have a pebble).},
author = {Dziembowski, Stefan and Faust, Sebastian and Kolmogorov, Vladimir and Pietrzak, Krzysztof},
doi = {10.1007/978-3-662-48000-7_29},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dziembowski et al. - 2015 - Proofs of space.pdf:pdf},
isbn = {9783662479995},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {585--605},
publisher = {Springer Verlag},
title = {{Proofs of space}},
volume = {9216},
year = {2015}
}
@article{Deb2021,
abstract = {An important feature of Proof-of-Work (PoW) blockchains is full dynamic availability, allowing miners to go online and offline while requiring only 50% of the online miners to be honest. Existing Proof-of-stake (PoS), Proof-of-Space and related protocols are able to achieve this property only partially, either requiring the additional assumption that adversary nodes are online from the beginning and no new adversary nodes come online afterwards, or use additional trust assumptions for newly joining nodes. We propose a new PoS protocol PoSAT which can provably achieve dynamic availability fully without any additional assumptions. The protocol is based on the longest chain and uses a Verifiable Delay Function for the block proposal lottery to provide an arrow of time. The security analysis of the protocol draws on the recently proposed technique of Nakamoto blocks as well as the theory of branching random walks. An additional feature of PoSAT is the complete unpredictability of who will get to propose a block next, even by the winner itself. This unpredictability is at the same level of PoW protocols, and is stronger than that of existing PoS protocols using Verifiable Random Functions.},
archivePrefix = {arXiv},
arxivId = {2010.08154},
author = {Deb, Soubhik and Kannan, Sreeram and Tse, David},
doi = {10.1007/978-3-662-64331-0_6},
eprint = {2010.08154},
isbn = {9783662643303},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {104--128},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{PoSAT: Proof-of-Work Availability and Unpredictability, Without the Work}},
volume = {12675 LNCS},
year = {2021}
}
@article{Song2018,
abstract = {In this thesis we study two major topics in cryptanalysis and optimization: software algebraic cryptanalysis and elliptic curve optimizations in cryptanalysis. The idea of algebraic cryptanalysis is to model a cipher by a Multivariate Quadratic (MQ) equation system. Solving MQ is an NP-hard problem. However, NP-hard problems have a point of phase transition where the problems become easy to solve. This thesis explores different optimizations to make solving algebraic cryptanalysis problems easier. We first worked on guessing a well-chosen number of key bits, a specific optimization problem leading to guess-then-solve attacks on GOST cipher. In addition to attacks, we propose two new security metrics of contradiction immunity and SAT immunity applicable to any cipher. These optimizations play a pivotal role in recent highly competitive results on full GOST. This and another cipher Simon, which we cryptanalyzed were submitted to ISO to become a global encryption standard which is the reason why we study the security of these ciphers in a lot of detail. Another optimization direction is to use well-selected data in conjunction with Plaintext/Ciphertext pairs following a truncated differential property. These allow to supplement an algebraic attack with extra equations and reduce solving time. This was a key innovation in our algebraic cryptanalysis work on NSA block cipher Simon and we could break up to 10 rounds of Simon64/128. The second major direction in our work is to inspect, analyse and predict the behaviour of ElimLin attack the complexity of which is very poorly understood, at a level of detail never seen before. Our aim is to extrapolate and discover the limits of such attacks, and go beyond with several types of concrete improvement. Finally, we have studied some optimization problems in elliptic curves which also deal with polynomial arithmetic over finite fields. We have studied existing implementations of the secp256k1 elliptic curve which is used in many popular cryptocurrency systems such as Bitcoin and we introduce an optimized attack on Bitcoin brain wallets and improved the state of art attack by 2.5 times.},
author = {Song, Guangyan},
journal = {   Doctoral thesis, UCL (University College London).   },
keywords = {book chapters,conference proceedings,digital web resources,discovery,journal articles,open access,open access repository,research,theses,ucl,ucl discovery,ucl library,ucl research},
month = {dec},
publisher = {UCL (University College London)},
title = {{Optimization and Guess-then-Solve Attacks in Cryptanalysis}},
year = {2018}
}
@article{Wetzels2016,
abstract = {In this document we present an overview of the background to and goals of the Password Hashing Competition (PHC) as well as the design of its winner, Argon2, and its security requirements and properties.},
archivePrefix = {arXiv},
arxivId = {1602.03097},
author = {Wetzels, Jos},
eprint = {1602.03097},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wetzels - 2016 - Open Sesame The Password Hashing Competition and Argon2.pdf:pdf},
journal = {IACR Cryptology ePrint Archive},
keywords = {argon2,functions,gpu-fpga-asic password cracking,memory-hard hash,password hashing competition},
month = {feb},
title = {{Open Sesame: The Password Hashing Competition and Argon2}},
url = {https://arxiv.org/pdf/1602.03097 http://arxiv.org/abs/1602.03097},
year = {2016}
}
@article{Barak2012,
abstract = {Informally, an obfuscator O is an (efficient, probabilistic) "compiler" that takes as input a program (or circuit) P and produces a new program O(P) that has the same functionality as P yet is "unintelligible" in some sense. Obfuscators, if they exist, would have a wide variety of cryptographic and complexity-theoretic applications, ranging from software protection to homomorphic encryption to complexity-theoretic analogues of Rice's theorem.Most of these applications are based on an interpretation of the "unintelligibility" condition in obfuscation as meaning that O(P) is a "virtual black box," in the sense that anything one can efficiently compute given O(P), one could also efficiently compute given oracle access to P. In this work, we initiate a theoretical investigation of obfuscation. Our main result is that, even under very weak formalizations of the above intuition, obfuscation is impossible. We prove this by constructing a family of efficient programs P that are unobfuscatable in the sense that (a) given any efficient program P that computes the same function as a program P $\epsilon$ P, the "source code" P can be efficiently reconstructed, yet (b) given oracle access to a (randomly selected) program P $\epsilon$ P, no efficient algorithm can reconstruct P (or even distinguish a certain bit in the code from random) except with negligible probability. We extend our impossibility result in a number of ways, including even obfuscators that (a) are not necessarily computable in polynomial time, (b) only approximately preserve the functionality, and (c) only need to work for very restricted models of computation (TC0).We also rule out several potential applications of obfuscators, by constructing "unobfuscatable" signature schemes, encryption schemes, and pseudorandom function families. {\textcopyright} 2012 ACM.},
author = {Barak, Boaz and Goldreich, Oded and Impagliazzo, Russell and Rudich, Steven and Ucla, Amit Sahai and Vadhan, Salil and Yang, Ke},
doi = {10.1145/2160158.2160159},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barak et al. - 2012 - On the (im)possibility of obfuscating programs.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
keywords = {Complexity theory,Cryptography,Homomorphic encryption,Pseudorandom functions,Rice's Theorem,Software protection,Software watermarking,Statistical zero knowledge},
month = {apr},
number = {2},
pages = {6},
title = {{On the (Im)possibility of obfuscating programs}},
volume = {59},
year = {2012}
}
@article{Guan2024,
abstract = {A sequential function is, informally speaking, a function f for which a massively parallel adversary cannot compute “substantially” faster than an honest user with limited parallel computation power. Sequential functions form the backbone of many primitives that are extensively used in blockchains such as verifiable delay functions (VDFs) and time-lock puzzles. Despite this widespread practical use, there has been little work studying the complexity or theory of sequential functions. Our main result is a black-box oracle separation between sequential functions and one-way functions: in particular, we show the existence of an oracle O that implies a sequential function but not a one-way function. This seems surprising since sequential functions are typically constructed from very strong assumptions that imply one-way functions and also since time-lock puzzles are known to imply one-way functions (Bitansky et al., ITCS '16). We continue our exploration of the theory of sequential functions. We show that, informally speaking, the decisional, worst-case variant of a certain class of sequential function called a continuous iterative sequential function (CISF) is PSPACE-complete. A CISF is, in a nutshell, a sequential function f that can be written in the form fk,x=gkx for some function g where k is an input determining the number of “rounds” the function is evaluated. We then show that more general forms of sequential functions are not contained in PSPACE relative to a random oracle. Given these results, we then ask if it is possible to build any interesting cryptographic primitives from sequential functions that are not one-way. It turns out that even if we assume just the existence of a CISF that is not one-way, we can build certain “fine-grained” cryptographic primitives where security is defined similarly to traditional primitives with the exception that it is only guaranteed for some (generally polynomial) amount of time. In particular, we show how to build “fine-grained” symmetric key encryption and “fine-grained” MACs from a CISF. We also show how to build fine-grained public-key encryption from a VDF with a few extra natural properties and indistinguishability obfuscation (iO) for null circuits. We do not assume one-way functions. Finally, we define a primitive that we call a commutative sequential function–essentially a sequential function that can be computed in sequence to get the same output in two different ways–and show that it implies fine-grained key exchange.},
author = {Guan, Jiaxin and Montgomery, Hart},
doi = {10.1007/978-3-031-68388-6_14},
isbn = {9783031683879},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {393--428},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{On Sequential Functions and Fine-Grained Cryptography}},
volume = {14924 LNCS},
year = {2024}
}
@article{Dottling2019,
abstract = {A proof of sequential work allows a prover to convince a verifier that a certain amount of sequential steps have been computed. In this work we introduce the notion of incremental proofs of sequential work where a prover can carry on the computation done by the previous prover incrementally, without affecting the resources of the individual provers or the size of the proofs. To date, the most efficient instance of proofs of sequential work [Cohen and Pietrzak, Eurocrypt 2018] for N steps require the prover to have (formula presented) memory and to run for (formula presented) steps. Using incremental proofs of sequential work we can bring down the prover's storage complexity to log N and its running time to N. We propose two different constructions of incremental proofs of sequential work: Our first scheme requires a single processor and introduces a poly-logarithmic factor in the proof size when compared with the proposals of Cohen and Pietrzak. Our second scheme assumes log N parallel processors but brings down the overhead of the proof size to a factor of 9. Both schemes are simple to implement and only rely on hash functions (modelled as random oracles).},
author = {D{\"{o}}ttling, Nico and Lai, Russell W.F. and Malavolta, Giulio},
doi = {10.1007/978-3-030-17656-3_11},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/D{\"{o}}ttling, Lai, Malavolta - 2019 - Incremental proofs of sequential work.pdf:pdf},
isbn = {9783030176556},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {292--323},
publisher = {Springer Verlag},
title = {{Incremental proofs of sequential work}},
volume = {11477 LNCS},
year = {2019}
}
@article{Liu2018,
abstract = {Time-lock encryption is a method to encrypt a message such that it can only be decrypted after a certain deadline has passed. We propose a novel time-lock encryption scheme, whose main advantage over prior constructions is that even receivers with relatively weak computational resources should immediately be able to decrypt after the deadline, without any interaction with the sender, other receivers, or a trusted third party. We build our time-lock encryption on top of the new concept of computational reference clocks and an extractable witness encryption scheme. We explain how to construct a computational reference clock based on Bitcoin. We show how to achieve constant level of multilinearity for witness encryption by using SNARKs. We propose a new construction of a witness encryption scheme which is of independent interest: our scheme, based on Subset-Sum, achieves extractable security without relying on obfuscation. The scheme employs multilinear maps of arbitrary order and is independent of the implementations of multilinear maps.},
author = {Liu, Jia and Jager, Tibor and Kakvi, Saqib A. and Warinschi, Bogdan},
doi = {10.1007/S10623-018-0461-X/FIGURES/1},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - How to build time-lock encryption.pdf:pdf},
issn = {15737586},
journal = {Designs, Codes, and Cryptography},
keywords = {Bitcoin,SNARKs,Time-lock encryption,Time-lock puzzles,Timed commitments,Timed-release encryption,Witness encryption},
month = {nov},
number = {11},
pages = {2549--2586},
publisher = {Springer New York LLC},
title = {{How to build time-lock encryption}},
url = {https://link.springer.com/article/10.1007/s10623-018-0461-x},
volume = {86},
year = {2018}
}
@article{Wesolowski2019,
abstract = {We construct a verifiable delay function (VDF). A VDF is a function whose evaluation requires running a given number of sequential steps, yet the result can be efficiently verified. They have applications in decentralised systems, such as the generation of trustworthy public randomness in a trustless environment, or resource-efficient blockchains. To construct our VDF, we actually build a trapdoor VDF. A trapdoor VDF is essentially a VDF which can be evaluated efficiently by parties who know a secret (the trapdoor). By setting up this scheme in a way that the trapdoor is unknown (not even by the party running the setup, so that there is no need for a trusted setup environment), we obtain a simple VDF. Our construction is based on groups of unknown order such as an RSA group, or the class group of an imaginary quadratic field. The output of our construction is very short (the result and the proof of correctness are each a single element of the group), and the verification of correctness is very efficient.},
author = {Wesolowski, Benjamin},
doi = {10.1007/978-3-030-17659-4_13},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wesolowski - 2019 - Efficient verifiable delay functions.pdf:pdf},
isbn = {9783030176587},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {379--407},
publisher = {Springer Verlag},
title = {{Efficient verifiable delay functions}},
volume = {11478 LNCS},
year = {2019}
}
@article{Cathalo2005,
abstract = {This paper revisits the important problem of sending a message "into the future" in such a way that no communication is needed between the server and other entities. This problem was recently re-investigated by Blake and Chan who showed a scalable non-interactive solution without considering a formal security model. We fill this gap by introducing a new stringent model tailored to the non-interactive setting. We then propose a new construction fitting our model and we show that it is more efficient than the recent non-interactive proposal (for which we also give a security proof in our model). We then explain how to provide our scheme and the one of Blake and Chan with an additional security property that strengthens the anonymity of receivers. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Cathalo, Julien and Libert, Beno{\^{i}}t and Quisquater, Jean Jacques},
doi = {10.1007/11602897_25},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cathalo, Libert, Quisquater - 2005 - Efficient and non-interactive timed-release encryption.pdf:pdf},
isbn = {3540309349},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Formal models,Provable security,Timed-release encryption},
pages = {291--303},
title = {{Efficient and non-interactive timed-release encryption}},
volume = {3783 LNCS},
year = {2005}
}
@article{Han2019,
abstract = {Blockchain technology has become extremely popular, during the last decade, mainly due to the successful application in the cryptocurrency domain. Following the explosion of Bitcoin and other cryptocurrencies, blockchain solutions are being deployed in almost every aspect of transactional operations as a means to safely exchange digital assets between non-trusted parties. At the heart of every blockchain deployment is the consensus protocol, which maintains the consistency of the blockchain upon satisfying incoming transactions. Although many consensus protocols have been recently introduced, the most prevalent is Proof-of- Work, which scales the blockchain globally by converting the consensus problem to a competition based on cryptographic hash functions; a process called 'mining'. The Proof-of- Work consensus protocol employs memory-hard algorithms in order to counteract ASIC or FPGA mining that may compromise the decentralization and democratization of the blockchain. Unfortunately, this leads to increased power consumption and scalability challenges since numerous processing units such as GPUs, FPGAs, and ASICs, are required to satisfy the ever-increasing operational requirements of blockchain deployments. In this paper, we perform an in-depth performance analysis and characterization of the most common memory-hard PoW algorithms running on NVIDIA GPUs. Motivated by our experimental findings, we apply a series of optimizations on Ethash algorithm, the consensus protocol of the Ethereum blockchain. The implemented optimizations accelerate performance by 14% and improve energy efficiency by 10% when executing on three NVIDIA GPUs. As a result, the optimized Ethash algorithm outperformed its fastest commercial implementation.},
author = {Han, Runchao and Foutris, Nikos and Kotselidis, Christos},
doi = {10.1109/ISPASS.2019.00011},
isbn = {9781728107462},
journal = {Proceedings - 2019 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2019},
keywords = {Blockchain,Crypto mining,Energy Efficiency,Ethereum,GPUs,Optimizations},
month = {apr},
pages = {22--33},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Demystifying Crypto-Mining: Analysis and Optimizations of Memory-Hard PoW Algorithms}},
year = {2019}
}
@article{Burdges2021,
abstract = {We introduce a new primitive named Delay Encryption, and give an efficient instantiation based on isogenies of supersingular curves and pairings. Delay Encryption is related to Time-lock Puzzles and Verifiable Delay Functions, and can be roughly described as “time-lock identity based encryption”. It has several applications in distributed protocols, such as sealed bid Vickrey auctions and electronic voting. We give an instantiation of Delay Encryption by modifying Boneh and Frankiln's IBE scheme, where we replace the master secret key by a long chain of isogenies, as in the isogeny VDF of De Feo, Masson, Petit and Sanso. Similarly to the isogeny-based VDF, our Delay Encryption requires a trusted setup before parameters can be safely used; our trusted setup is identical to that of the VDF, thus the same parameters can be generated once and shared for many executions of both protocols, with possibly different delay parameters. We also discuss several topics around delay protocols based on isogenies that were left untreated by De Feo et al., namely: distributed trusted setup, watermarking, and implementation issues.},
author = {Burdges, Jeffrey and {De Feo}, Luca},
doi = {10.1007/978-3-030-77870-5_11},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burdges, De Feo - 2021 - Delay Encryption.pdf:pdf},
isbn = {9783030778699},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Delay functions,Isogenies,Pairings,Supersingular elliptic curves},
pages = {302--326},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Delay Encryption}},
volume = {12696 LNCS},
year = {2021}
}
@article{Ephraim2020,
abstract = {We introduce the notion of a continuous verifiable delay function (cVDF): a function g which is (a) iteratively sequential—meaning that evaluating the iteration g(t) of g (on a random input) takes time roughly t times the time to evaluate g, even with many parallel processors, and (b) (iteratively) verifiable—the output of g(t) can be efficiently verified (in time that is essentially independent of t). In other words, the iterated function g(t) is a verifiable delay function (VDF) (Boneh et al., CRYPTO '18), having the property that intermediate steps of the computation (i.e., g (t') for t'< t) are publicly and continuously verifiable. We demonstrate that cVDFs have intriguing applications: (a) they can be used to construct public randomness beacon that only require an initial random seed (and no further unpredictable sources of randomness), (b) enable outsourceable where any part of the VDF computation can be verifiably outsourced, and (c) have deep complexity-theoretic consequences: in particular, they imply the existence of depth-robust moderately-hard Nash equilibrium problem instances, i.e. instances that can be solved in polynomial time yet require a high sequential running time. Our main result is the construction of a cVDF based on the repeated squaring assumption and the soundness of the Fiat-Shamir (FS) heuristic for constant-round proofs. We highlight that when viewed as a (plain) VDF, our construction requires a weaker FS assumption than previous ones (earlier constructions require the FS heuristic for either super-logarithmic round proofs, or for arguments).},
author = {Ephraim, Naomi and Freitag, Cody and Komargodski, Ilan and Pass, Rafael},
doi = {10.1007/978-3-030-45727-3_5},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ephraim et al. - 2020 - Continuous verifiable delay functions.pdf:pdf},
isbn = {9783030457266},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {125--154},
publisher = {Springer},
title = {{Continuous verifiable delay functions}},
url = {https://link.springer.com/chapter/10.1007/978-3-030-45727-3_5},
volume = {12107 LNCS},
year = {2020}
}
@article{Choe2019,
abstract = {In a traditional DRAM-based main memory architecture, a memory access operation requires much more time and energy than a simple logic operation. This fact is exploited to build time-consuming and power-hungry memory-hard cryptographic functions that serve the purpose of hindering brute-force security attacks. The security of such memory-hard functions depends entirely on the non-trivial costs of memory access. However, various compute-capable memory technologies have recently emerged as promising ways to reduce the memory access bottleneck, yet no one has looked into how they may impact the security of memory-hard cryptographic functions. In this preliminary work, we investigate the impact of near-data-processing (NDP) on scrypt, a widely used memory-hard password-based key-derivation function, and discuss the opportunities to further undermine scrypt using compute-capable memory.},
author = {Choe, Jiwon and Moreshet, Tali and Bahar, R. Iris and Herlihy, Maurice},
doi = {10.1145/3357526.3357570},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choe et al. - 2019 - Attacking Memory-Hard scrypt with Near-Data-Processing.pdf:pdf},
isbn = {9781450372060},
journal = {ACM International Conference Proceeding Series},
month = {sep},
pages = {33--37},
publisher = {Association for Computing Machinery},
title = {{Attacking Memory-Hard scrypt with Near-Data-Processing}},
url = {https://dl.acm.org/doi/pdf/10.1145/3357526.3357570},
year = {2019}
}
@inproceedings{7467361,
abstract = {We present a new hash function Argon2, which is oriented at protection of low-entropy secrets without secret keys. It requires a certain (but tunable) amount of memory, imposes prohibitive time-memory and computation-memory tradeoffs on memory-saving users, and is exceptionally fast on regular PC. Overall, it can provide ASIC-and botnet-resistance by filling the memory in 0.6 cycles per byte in the non-compressible way.},
author = {Biryukov, Alex and Dinu, Daniel and Khovratovich, Dmitry},
booktitle = {Proceedings - 2016 IEEE European Symposium on Security and Privacy, EURO S and P 2016},
doi = {10.1109/EuroSP.2016.31},
isbn = {9781509017515},
keywords = {Argon2,memory-hard,passwords},
pages = {292--302},
title = {{Argon2: New generation of memory-hard functions for password hashing and other applications}},
year = {2016}
}
@article{Cheng2018,
abstract = {Bitcoins cryptocurrency is a decentralized digital currency released as open-source software in 2009. In these 9 years of development, the mining devices are changed from the public CPUs into a professional ASICs. The ASICs is a application-specific integrated circuit, which is designed exclusively for mining. Because of its outstanding computing power, the ASICs gradually replaced other devices and monopolies the Bitcoin market. In this paper, we propose a new blockchain chain protocol, which allows us to adjust the loading time of the mining process. This strategy reduces the ASIC and CPU mining rate of the gap. So that the using of ASIC mining is limited. Finally, we analyzed the safety of this new chain structure and compared it to the original chain protocol.},
author = {Cheng, Sui and Lin, Sian Jheng},
doi = {10.1109/SEGE.2018.8499411},
isbn = {9781538664100},
journal = {2018 6th IEEE International Conference on Smart Energy Grid Engineering, SEGE 2018},
keywords = {Blockchain,Memory-hard function,Multiple related protocols},
month = {oct},
pages = {284--287},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Memory-Hard Blockchain Protocol}},
year = {2018}
}
@article{Lai2019,
abstract = {To make a time capsule on the Internet, which will be opened at a planned time in the future, without third parties' involvement has always been a difficult problem. Although there are many researches worked on various time-lock systems, they may have some shortcomings like uncertainty in decryption time, not fully decentralized, hard to estimate the required computing resources. In this paper, we proposed a protocol and a reliable encryption scheme to make time-sensitive message be opened on time at a fully decentralized environment, which is then integrated with the blockchain to adapt to different computing power situations. The method also provides the capability of incorporating with appropriate incentives for encouraging participants to contribute their computing resources, which makes our system more suitable for real world applications.},
author = {Lai, Wei Jr and Hsueh, Chih Wen and Wu, Ja Ling},
doi = {10.1109/Blockchain.2019.00047},
isbn = {9781728146935},
journal = {Proceedings - 2019 2nd IEEE International Conference on Blockchain, Blockchain 2019},
keywords = {Blockchain,Privacy Preserving,Proof of Work,Time-Lock Encryption},
month = {jul},
pages = {302--307},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A fully decentralized time-lock encryption system on blockchain}},
year = {2019}
}
@article{Damgard1990,
abstract = {We show that if there exists a computationally collision free function f from m bits to t bits where m > t, then there exists a computationally collision free function h mapping messages of arbitrary polynomial lengths to t-bit strings. Let n be the length of the message. h can be constructed either such that it can be evaluated in time linear in n using 1 processor, or such that it takes time O(log(n)) using O(n) processors, counting evaluations of f as one step. Finally, for any constant k and large n, a speedup by a factor of k over the first construction is available using k processors. Apart from suggesting a generally sound design principle for hash functions, our results give a unified view of several apparently unrelated constructions of hash functions proposed earlier. It also suggests changes to other proposed constructions to make a proof of security potentially easier. We give three concrete examples of constructions, based on modular squaring, on Wolfram's pseudoranddom bit generator [Wo], and on the knapsack problem.},
author = {Damg{\aa}rd, Ivan Bjerre},
doi = {10.1007/0-387-34805-0_39},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Damg{\aa}rd - 1990 - A design principle for hash functions.pdf:pdf},
isbn = {9780387973173},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {416--427},
publisher = {Springer Verlag},
title = {{A design principle for hash functions}},
volume = {435 LNCS},
year = {1990}
}
@article{Nguyen2019,
abstract = {Today's computing architectures suffer from the three well-known bottlenecks, which are the memory, the power and the instruction-level parallelism walls. Emerging non-volatile technologies, such as memristor, enable new resistive architectures that alleviate at least two of such bottlenecks, as they can process data within the memory with almost no leakage. In this paper, we propose a novel resistive computing architecture by extending a conventional architecture with a resistive based Computation-In-Memory accelerator (CIMX). We evaluate the delay, energy and area of the conventional and CIMX architecture using an analytical model and a simulation framework. The results (both based on the analytical model and simulation framework) show that the proposed architecture achieves at least one order of magnitude improvement in terms of performance, area, and energy efficiency for the considered benchmarks.},
author = {Nguyen, Hoang Anh Du and Yu, Jintao and Lebdeh, Muath Abu and Taouil, Mottaqiallah and Hamdioui, Said},
doi = {10.1145/3357526.3357554},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2019 - A computation-in-memory accelerator based on resistive devices.pdf:pdf},
isbn = {9781450372060},
journal = {ACM International Conference Proceeding Series},
keywords = {Accelerator,Computation-in-Memory,Memristor,Resistive Computing},
month = {sep},
pages = {19--32},
publisher = {Association for Computing Machinery},
title = {{A computation-in-memory accelerator based on resistive devices}},
year = {2019}
}
@article{Shannon1948,
author = {Shannon, C. E.},
doi = {10.1002/j.1538-7305.1948.tb01338.x},
issn = {15387305},
journal = {Bell System Technical Journal},
mendeley-groups = {GeoVault/brainwallets},
month = {jul},
number = {3},
pages = {379--423},
publisher = {John Wiley & Sons, Ltd},
title = {{A Mathematical Theory of Communication}},
url = {/doi/pdf/10.1002/j.1538-7305.1948.tb01338.x https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1948.tb01338.x},
volume = {27},
year = {1948}
}
@misc{bip39,
  title        = {BIP 39: Mnemonic code for generating deterministic keys},
  author       = {Marek Palatinus and Pavol Rusnak and Aaron Voisine and Sean Bowe},
  year         = {2013},
  howpublished = {\url{https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki}},
  note         = {Bitcoin Improvement Proposals},
  version      = {Final}
}

@article{Durmuth2015,
abstract = {Passwords are still by far the most widely used form of user authentication, for applications ranging from online banking or corporate network access to storage encryption. Password guessing thus poses a serious threat for a multitude of applications. Modern password hashes are specifically designed to slow down guessing attacks. However, having exact measures for the rate of password guessing against determined attackers is non-trivial but important for evaluating the security for many systems. Moreover, such information may be valuable for designing new password hashes, such as in the ongoing password hashing competition (PHC). In this work, we investigate two popular password hashes, bcrypt and scrypt, with respect to implementations on non-standard computing platforms. Both functions were specifically designed to only allow slow-rate password derivation and, thus, guessing rates. We develop a methodology for fairly comparing different implementations of password hashes, and apply this methodology to our own implementation of scrypt on GPUs, as well as existing implementations of bcrypt and scrypt on GPUs and FPGAs.},
author = {D{\"{u}}rmuth, Markus and Kranz, Thorsten},
doi = {10.1007/978-3-319-24192-0_2},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bcrypt,Efficient implementations,FPGAs,GPUs,Password cracking,Password hashing,Scrypt},
mendeley-groups = {GeoVault/brainwallets},
pages = {19--38},
publisher = {Springer Verlag},
title = {{On password guessing with GPUs and FPGAs}},
volume = {9393 LNCS},
year = {2015}
}
@misc{Qiu2016,
abstract = {Due to the development of GPGPU (General Purpose Graphic Processing Unit) technology, GPU has been applied in many computation tasks as accelerators. In this paper, a new password recovery technique for the standardized hash functions, MD5 and SHA1, are proposed by combining the optimization methods on GPU. The performance on AMD HD7970 is 2615 mc/s for SHA1 and 6877 mc/s for MD5, which is 10 times better than the original implementation. If the length of password is limited, our GPUbased technique makes it possible to recover password from hash values in a reasonable time.},
author = {Qiu, Weidong and Gong, Zheng and Guo, Yidong and Liu, Bozhong and Tang, Xiaoming and Yuan, Yuheng},
booktitle = {Journal of Information Science and Engineering},
issn = {10162364},
keywords = {GPU,Hash functions,Opencl,Optimization,Password recovery},
mendeley-groups = {GeoVault/brainwallets},
number = {1},
pages = {97--112},
title = {{GPU-based high performance password recovery technique for hash functions}},
url = {https://www.researchgate.net/publication/292761539_GPU-Based_High_Performance_Password_Recovery_Technique_for_Hash_Functions},
urldate = {2025-07-01},
volume = {32},
year = {2016}
}
@article{Choi2024,
abstract = {Currently, cryptographic hash functions are widely used in various applications, including message authentication codes, cryptographic random generators, digital signatures, key derivation functions, and post-quantum algorithms. Notably, they play a vital role in establishing secure communication between servers and clients. Specifically, servers often need to compute a large number of hash functions simultaneously to provide smooth services to connected clients. In this paper, we present highly optimized parallel implementations of Lightweight Secure Hash (LSH), a hash algorithm developed in Korea, on server sides. To optimize LSH performance, we leverage two parallel architectures: AVX-512 on high-end CPUs and NVIDIA GPUs. In essence, we introduce a word-level parallel processing design suitable for AVX-512 instruction sets and a data parallel processing design appropriate for the NVIDIA CUDA platform. In the former approach, we parallelize the core functions of LSH using AVX-512 registers and instructions. As a result, our first implementation achieves a performance improvement of up to 50.37% compared to the latest LSH AVX-2 implementation. In the latter approach, we optimize the core operation of LSH with CUDA PTX assembly and apply a coalesced memory access pattern. Furthermore, we determine the optimal number of blocks/threads configuration and CUDA streams for RTX 2080Ti and RTX 3090. Consequently, in the RTX 3090 architecture, our optimized CUDA implementation achieves about a 180.62% performance improvement compared with the initially ported LSH implementation to the CUDA platform. As far as we know, this is the first work on optimizing LSH with AVX-512 and NVIDIA GPU. The proposed implementation methodologies can be used alone or together in a server environment to achieve the maximum throughput of LSH computation.},
author = {Choi, Hojin and Choi, Seong Jun and Seo, Seog Chung},
doi = {10.3390/electronics13050896},
file = {:C\:/Users/Marko/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Choi, Seo - 2024 - Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments.pdf:pdf},
issn = {20799292},
journal = {Electronics (Switzerland)},
keywords = {AVX-512,CUDA,GPU,SIMD,hash function,parallel processing},
mendeley-groups = {GeoVault/brainwallets},
month = {feb},
number = {5},
pages = {896},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments}},
url = {https://www.mdpi.com/2079-9292/13/5/896/htm https://www.mdpi.com/2079-9292/13/5/896},
volume = {13},
year = {2024}
}

@misc{RFC9106,
  author       = {Biryukov, Alex and Dinu, Daniel and Khovratovich, Dmitry},
  title        = {{Argon2} Memory-Hard Function for Password Hashing and Proof-of-Work Applications},
  year         = {2021},
  howpublished = {RFC 9106},
  institution  = {Internet Engineering Task Force (IETF)},
  url          = {https://www.rfc-editor.org/rfc/rfc9106},
  urldate      = {2026-01-15}
}


@inproceedings{barros_memory-hard_2023,
	address = {Brasil},
	title = {A {Memory}-{Hard} {Function} for {Password} {Hashing} and {Key} {Derivation}},
	url = {https://sol.sbc.org.br/index.php/sbseg/article/view/27233},
	doi = {10.5753/sbseg.2023.232827},
	abstract = {Key derivation and password scrambling are crucial procedures in cryptographic applications, and the security of these methods against brute-force attacks is a critical concern in face of the increasing computational power available to perform these attacks. This paper proposes a candidate memory-hard function for password scrambling and key derivation, based on some design principles such as flexibility, variable-length output, adjustable parametrization to achieve high cache miss rates and dynamic update of the internal buffer.},
	urldate = {2026-01-15},
	booktitle = {Anais do {XXIII} {Simpósio} {Brasileiro} de {Segurança} da {Informação} e de {Sistemas} {Computacionais} ({SBSeg} 2023)},
	publisher = {Sociedade Brasileira de Computação - SBC},
	author = {Barros, Charles F. De},
	month = sep,
	year = {2023},
	pages = {522--527},
}

@article{eum_optimized_2023,
	title = {Optimized {Implementation} of {Argon2} {Utilizing} the {Graphics} {Processing} {Unit}},
	volume = {13},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/13/16/9295},
	doi = {10.3390/app13169295},
	abstract = {In modern information technology systems, secure storage and transmission of personal and sensitive data are recognized as important tasks. These requirements are achieved through secure and robust encryption methods. Argon2 is an advanced cryptographic algorithm that emerged as the winner in the Password Hashing Competition (PHC), offering a concrete and secure measure. Argon2 also provides a secure mechanism against side-channel attacks and cracking attacks using parallel processing (e.g., GPU). In this paper, we analyze the existing GPU-based implementation of the Argon2 algorithm and further optimize the implementation by improving the performance of the hashing function during the computation process. The proposed method focuses on enhancing performance by distributing tasks between CPU and GPU units, reducing the data transfer cost for efficient GPU-based parallel processing. By shifting several stages from the CPU to the GPU, the data transfer cost is significantly reduced, resulting in faster processing times, particularly when handling a larger number of passwords and higher levels of parallelism. Additionally, we optimize the utilization of the GPU’s shared memory, which enhances memory access speed, especially in the computation of the hash value generation process. Furthermore, we leverage the parallel processing capabilities of the GPU to perform efficient brute-force attacks. By computing the H function on the GPU, the proposed implementation can generate initial blocks for multiple inputs in a single operation, making brute-force attacks in an efficient way. The proposed implementation outperforms existing methods, especially when processing a larger number of passwords and operating at higher levels of parallelism.},
	language = {en},
	number = {16},
	urldate = {2026-01-15},
	journal = {Applied Sciences},
	author = {Eum, Siwoo and Kim, Hyunjun and Song, Minho and Seo, Hwajeong},
	month = aug,
	year = {2023},
	pages = {9295},
}
@article{Bonneau2012,
abstract = {We report on the largest corpus of user-chosen passwords ever studied, consisting of anonymized password histograms representing almost 70 million Yahoo! users, mitigating privacy concerns while enabling analysis of dozens of subpopulations based on demographic factors and site usage characteristics. This large data set motivates a thorough statistical treatment of estimating guessing difficulty by sampling from a secret distribution. In place of previously used metrics such as Shannon entropy and guessing entropy, which cannot be estimated with any realistically sized sample, we develop partial guessing metrics including a new variant of guesswork parameterized by an attacker's desired success rate. Our new metric is comparatively easy to approximate and directly relevant for security engineering. By comparing password distributions with a uniform distribution which would provide equivalent security against different forms of guessing attack, we estimate that passwords provide fewer than 10 bits of security against an online, trawling attack, and only about 20 bits of security against an optimal offline dictionary attack. We find surprisingly little variation in guessing difficulty; every identifiable group of users generated a comparably weak password distribution. Security motivations such as the registration of a payment card have no greater impact than demographic factors such as age and nationality. Even proactive efforts to nudge users towards better password choices with graphical feedback make little difference. More surprisingly, even seemingly distant language communities choose the same weak passwords and an attacker never gains more than a factor of 2 efficiency gain by switching from the globally optimal dictionary to a population-specific lists. {\textcopyright} 2012 IEEE.},
author = {Bonneau, Joseph},
doi = {10.1109/SP.2012.49},
file = {:C\:/Users/Marko/Downloads/The_Science_of_Guessing_Analyzing_an_Anonymized_Corpus_of_70_Million_Passwords.pdf:pdf},
isbn = {9780769546810},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
keywords = {authentication,computer security,data mining,information theory,statistics},
mendeley-groups = {GeoVault/brainwallets},
pages = {538--552},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{The science of guessing: Analyzing an anonymized corpus of 70 million passwords}},
url = {https://ieeexplore.ieee.org/abstract/document/6234435},
year = {2012}
}

@book{WGS84,
  title={Department of Defense World Geodetic System 1984: its definition and relationships with local geodetic systems},
  author={United States. Defense Mapping Agency},
  volume={8350},
  year={1987},
  publisher={Defense Mapping Agency}
}

@article{Venter2016,
  author = {Venter, Oscar and others},
  title = {Sixteen years of change in the global terrestrial human footprint},
  journal = {Nature Communications},
  year = {2016},
  volume = {7},
  pages = {12558}
}

@article{Konkle2010,
abstract = {Observers can store thousands of object images in visual long-term memory with high fidelity, but the fidelity of scene representations in long-term memory is not known. Here, we probed scene-representation fidelity by varying the number of studied exemplars in different scene categories and testing memory using exemplar-level foils. Observers viewed thousands of scenes over 5.5 hr and then completed a series of forced-choice tests. Memory performance was high, even with up to 64 scenes from the same category in memory. Moreover, there was only a 2% decrease in accuracy for each doubling of the number of studied scene exemplars. Surprisingly, this degree of categorical interference was similar to the degree previously demonstrated for object memory. Thus, although scenes have often been defined as a superset of objects, our results suggest that scenes and objects may be entities at a similar level of abstraction in visual long-term memory. {\textcopyright} The Author(s) 2010.},
author = {Konkle, Talia and Brady, Timothy F. and Alvarez, George A. and Oliva, Aude},
doi = {10.1177/0956797610385359},
file = {:C\:/Users/Marko/Downloads/konkle-et-al-2010-scene-memory-is-more-detailed-than-you-think.pdf:pdf},
issn = {1467-9280},
journal = {Psychological science},
keywords = {Adult,Attention*,Aude Oliva,Choice Behavior,Discrimination,Female,Humans,MEDLINE,Male,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-P.H.S.,Non-U.S. Gov't,PMC3397240,Pattern Recognition,Practice,Psychological,Psychological*,Psychology,Psychology*,PubMed Abstract,Recognition,Research Support,Retention,Talia Konkle,Timothy F Brady,U.S. Gov't,Visual*,Young Adult,doi:10.1177/0956797610385359,pmid:20921574},
mendeley-groups = {GeoVault/brainwallets},
month = {nov},
number = {11},
pages = {1551--1556},
pmid = {20921574},
publisher = {Psychol Sci},
title = {{Scene memory is more detailed than you think: the role of categories in visual long-term memory}},
url = {https://pubmed.ncbi.nlm.nih.gov/20921574/},
volume = {21},
year = {2010}
}
@article{Montello1998,
author = {Montello, D.},
file = {:C\:/Users/Marko/Downloads/konkle-et-al-2010-scene-memory-is-more-detailed-than-you-think.pdf:pdf;:C\:/Users/Marko/Downloads/microgenesis.pdf:pdf},
mendeley-groups = {GeoVault/brainwallets},
title = {{A New Framework for Understanding the Acquisition of Spatial Knowledge in Large-Scale Environments}},
year = {1998}
}

@article{McNamara2012,
abstract = {The goal of this chapter is to review empirical and theoretical advancements in the scientific understanding of human spatial memory. I attend primarily to spatial memories acquired from direct experience, such as vision and locomotion, and on spaces sufficiently large to afford movement, such as translation and rotation, although I take the liberty of referring to a few studies that investigated memories of table-top-sized "environments." This chapter is divided into six principal sections. I begin by discussing the elemental types of spatial knowledge: object–place knowledge, route knowledge, environmental shape knowledge, and survey knowledge. In the second section, I investigate classical and current theories of the acquisition of spatial knowledge. The third section discusses properties of spatial knowledge, such as its hierarchical structure and orientation dependence. In the fourth section, I examine the concept of spatial reference systems and the nature of the reference systems used in spatial memory. I then review contemporary cognitive models of spatial memory, with an eye for identifying similarities among them. I close the chapter with a summary and prospectus for future research on human spatial memory. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
author = {McNamara, Timothy P.},
doi = {10.1037/13936-010},
journal = {Handbook of spatial cognition.},
mendeley-groups = {GeoVault/brainwallets},
month = {oct},
pages = {173--190},
publisher = {American Psychological Association},
title = {{Spatial memory: Properties and organization.}},
year = {2012}
}

@article{Bonneau2012a,
abstract = {We evaluate two decades of proposals to replace text passwords for general-purpose user authentication on the web using a broad set of twenty-five usability, deployability and security benefits that an ideal scheme might provide. The scope of proposals we survey is also extensive, including password management software, federated login protocols, graphical password schemes, cognitive authentication schemes, one-time passwords, hardware tokens, phone-aided schemes and biometrics. Our comprehensive approach leads to key insights about the difficulty of replacing passwords. Not only does no known scheme come close to providing all desired benefits: none even retains the full set of benefits that legacy passwords already provide. In particular, there is a wide range from schemes offering minor security benefits beyond legacy passwords, to those offering significant security benefits in return for being more costly to deploy or more difficult to use. We conclude that many academic proposals have failed to gain traction because researchers rarely consider a sufficiently wide range of real-world constraints. Beyond our analysis of current schemes, our framework provides an evaluation methodology and benchmark for future web authentication proposals. {\textcopyright} 2012 IEEE.},
author = {Bonneau, Joseph and Herley, Cormac and {Van Oorschot}, Paul C. and Stajano, Frank},
doi = {10.1109/SP.2012.44},
isbn = {9780769546810},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
keywords = {authentication,computer security,deployability,economics,human computer interaction,security and usability,software engineering},
mendeley-groups = {GeoVault/brainwallets},
pages = {553--567},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{The quest to replace passwords: A framework for comparative evaluation of web authentication schemes}},
url = {https://ieeexplore.ieee.org/document/6234436},
year = {2012}
}









